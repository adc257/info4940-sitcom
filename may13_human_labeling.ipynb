{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adc257/info4940-sitcom/blob/main/may13_human_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U-WsE7AlB33",
        "outputId": "6c057b65-318a-4fff-c82a-7b6f175acd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTztUAKRlp7E",
        "outputId": "54768310-581a-482d-d049-b22a04728b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "X6d94esWlNn7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adc257/info4940-sitcom.git\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiCa_A65lKVN",
        "outputId": "2ad43d81-825e-48ae-cf60-c30115bfb992"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'info4940-sitcom' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining some helper functions"
      ],
      "metadata": {
        "id": "kwyri_r4mFhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files(start_path):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(start_path):\n",
        "        for file in files:\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    file_paths.sort()\n",
        "    return file_paths\n",
        "\n",
        "def fPathList_TO_DialogueWithContext(filePathList, nSentenceContext):\n",
        "  humorous_inputs = []\n",
        "  non_humorous_inputs = []\n",
        "\n",
        "  # formatted_inputs = []\n",
        "  # labels = []\n",
        "\n",
        "  for episodePath in filePathList:\n",
        "\n",
        "    with open(episodePath, 'r') as file:\n",
        "      data = json.load(file)\n",
        "      data = [info for (_, info) in data.items()]\n",
        "\n",
        "\n",
        "      i = nSentenceContext\n",
        "      while i < len(data):\n",
        "        context = [data[j] for j in range(i-nSentenceContext, i)]\n",
        "        target = data[i]\n",
        "\n",
        "        if 'isHumor' in target:\n",
        "          humorous_inputs.append(transformInput(context, target))\n",
        "        else:\n",
        "          non_humorous_inputs.append(transformInput(context, target))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "  return humorous_inputs, non_humorous_inputs\n",
        "\n",
        "def sampleEven(humorousData, nonHumorousData, sample_size_per_class, rand_seed):\n",
        "  sample_size_per_class = min(sample_size_per_class, len(humorousData), len(nonHumorousData))\n",
        "\n",
        "  sample_inputs = []\n",
        "  sample_labels = []\n",
        "\n",
        "  data = humorousData\n",
        "  test_size = (len(data) - sample_size_per_class) / len(data)\n",
        "  sample, _ = train_test_split(data, test_size=test_size, random_state=rand_seed)\n",
        "  sample_inputs += sample\n",
        "  sample_labels += [1]*len(sample)\n",
        "\n",
        "\n",
        "  data = nonHumorousData\n",
        "  test_size = (len(data) - sample_size_per_class) / len(data)\n",
        "  sample, _ = train_test_split(data, test_size=test_size, random_state=rand_seed)\n",
        "  sample_inputs += sample\n",
        "  sample_labels += [0]*len(sample)\n",
        "\n",
        "  return sample_inputs, sample_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "BT2c3Cm_mFuB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testGPT(inputs, labels, printResultsEveryN=50):\n",
        "  resultsData = {\n",
        "      \"truePos\": [],\n",
        "      \"trueNeg\": [],\n",
        "      \"falsePos\": [],\n",
        "      \"falseNeg\": [],\n",
        "      \"other\": [],\n",
        "  }\n",
        "\n",
        "  results = {\n",
        "      \"truePos\": 0,\n",
        "      \"trueNeg\": 0,\n",
        "      \"falsePos\": 0,\n",
        "      \"falseNeg\": 0,\n",
        "      \"other\": 0,\n",
        "  }\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  pred = []\n",
        "\n",
        "\n",
        "\n",
        "  # terminators = [\n",
        "  #     tokenizer.eos_token_id,\n",
        "  #     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  # ]\n",
        "\n",
        "  for i in range(len(inputs)):\n",
        "    input_ = inputs[i]\n",
        "    label = labels[i]\n",
        "\n",
        "    # input_ids = tokenizer.apply_chat_template(\n",
        "    #   input,\n",
        "    #   add_generation_prompt=True,\n",
        "    #   return_tensors=\"pt\"\n",
        "    # ).to(model.device)\n",
        "\n",
        "    # outputs = model.generate(\n",
        "    #   input_ids,\n",
        "    #   max_new_tokens=50,\n",
        "    #   eos_token_id=terminators,\n",
        "    #   do_sample=True,\n",
        "    #   temperature=0.6,\n",
        "    #   top_p=0.9,\n",
        "    #   num_return_sequences=1,\n",
        "    # )\n",
        "\n",
        "    # response = outputs[0][input_ids.shape[-1]:]\n",
        "    # response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=input_\n",
        "    )\n",
        "\n",
        "    response = completion.choices[0].message.content\n",
        "\n",
        "\n",
        "    if str(response) == str(label):\n",
        "      # true\n",
        "      if str(label) == \"1\":\n",
        "        # pos\n",
        "        resultsData[\"truePos\"].append(input_)\n",
        "        results[\"truePos\"] += 1\n",
        "      elif str(label) == \"0\":\n",
        "        # neg\n",
        "        resultsData[\"trueNeg\"].append(input_)\n",
        "        results[\"trueNeg\"] += 1\n",
        "      else:\n",
        "        # other\n",
        "        resultsData[\"other\"].append(input_)\n",
        "        results[\"other\"] += 1\n",
        "      correct += 1\n",
        "    else:\n",
        "      # false\n",
        "      if str(label) == \"1\":\n",
        "        # pos, pred neg\n",
        "        resultsData[\"falseNeg\"].append(input_)\n",
        "        results[\"falseNeg\"] += 1\n",
        "      elif str(label) == \"0\":\n",
        "        # neg, pred pos\n",
        "        resultsData[\"falsePos\"].append(input_)\n",
        "        results[\"falsePos\"] += 1\n",
        "      else:\n",
        "        # other\n",
        "        resultsData[\"other\"].append(input_)\n",
        "        results[\"other\"] += 1\n",
        "    pred.append(response)\n",
        "    total += 1\n",
        "\n",
        "    if i % printResultsEveryN == 0:\n",
        "      print(f\"Test: {i+1} of  {len(inputs)}\")\n",
        "      print(f\"Success rate: {correct / total:.2%}\\n\")\n",
        "      print(\"current results:\", results)\n",
        "\n",
        "\n",
        "  print(f\"Final success rate: {correct / total:.2%}\")\n",
        "  print(\"final results:\", results)\n",
        "\n",
        "  # true = [str(i) for i in labels]\n",
        "  report = None # classification_report(y_pred=pred, y_true=true, target_names=[\"Non-Humorous\",\"Humorous\"])\n",
        "\n",
        "  return results, resultsData, pred, report"
      ],
      "metadata": {
        "id": "dx122flnPV9d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining input formatter"
      ],
      "metadata": {
        "id": "NV9T5H7RoN2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformInput(context, target):\n",
        "  messages = []\n",
        "\n",
        "  separator = \"\\n\"\n",
        "  quotation_token = '\"'\n",
        "\n",
        "  # formatted_input = \"\"\n",
        "\n",
        "  if (len(context) > 0):\n",
        "    # if context...\n",
        "    systemPrmpt = {\n",
        "        'role': 'system',\n",
        "        'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "    }\n",
        "  else:\n",
        "    # if no context...\n",
        "    systemPrmpt = {\n",
        "        'role': 'system',\n",
        "        'content': \"You are humor classification model that takes a given line and determines whether it was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "    }\n",
        "\n",
        "  # append systemPrmpt\n",
        "  messages.append(systemPrmpt)\n",
        "\n",
        "  systemPrmpt2 = {\n",
        "      'role': 'system',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  userPrmpt = {\n",
        "      'role': 'user',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  userPrmpt2 = {\n",
        "      'role': 'user',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  asstPrmpt = {\n",
        "      'role': 'assistant',\n",
        "      'content': \"\"\n",
        "  }\n",
        "\n",
        "  # append userPrmpt\n",
        "  # messages.append(userPrmpt)\n",
        "\n",
        "  if len(context) > 0:\n",
        "    # asstPrmpt['content'] += \"Can you provide some context to help me with this decision?\"\n",
        "    # append asstPrmpt\n",
        "    # messages.append(asstPrmpt)\n",
        "\n",
        "    userPrmpt['content'] += \"Use the following as context:\" + separator*2\n",
        "\n",
        "    # if entry['Scene'] != current_scene:\n",
        "    #   current_scene = entry[\"Scene\"]\n",
        "    # userPrmpt['content'] += \"The scene changes to: \"\n",
        "    # else:\n",
        "\n",
        "    entry = context[0]\n",
        "    current_scene = entry[\"Scene\"]\n",
        "    userPrmpt['content'] += \"The Scene is: \"\n",
        "    userPrmpt['content'] += current_scene + separator\n",
        "\n",
        "    for entry in context:\n",
        "      if current_scene != entry['Scene']:\n",
        "        current_scene = entry[\"Scene\"]\n",
        "        userPrmpt['content'] += \"The scene changes to: \" + current_scene + separator\n",
        "\n",
        "      recipients = entry['Recipients']\n",
        "      recipients_str = \"\"\n",
        "      if len(recipients) == 0:\n",
        "        recipients_str += \"to themselves\"\n",
        "      elif len(recipients) == 1:\n",
        "        recipients_str += recipients[0]\n",
        "      else:\n",
        "        recipients_str += \", \".join(recipients[:-1]) + \" and \" + recipients[-1]\n",
        "\n",
        "      userPrmpt['content'] += f\"{entry['Speaker']} says {quotation_token}{entry['Dialogue']}{quotation_token} to {recipients_str}.\"\n",
        "      userPrmpt['content'] += separator\n",
        "\n",
        "    # append userPrmpt2\n",
        "\n",
        "  entry = target\n",
        "  if len(context) > 0:\n",
        "    # with context\n",
        "    userPrmpt['content'] += separator * 2 + \"Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\"\n",
        "\n",
        "    current_scene = context[-1]['Scene']\n",
        "    userPrmpt['content'] += separator*2\n",
        "\n",
        "\n",
        "    if entry['Scene'] != current_scene:\n",
        "      current_scene = entry[\"Scene\"]\n",
        "      userPrmpt['content'] += \"The scene changes to: \"\n",
        "    else:\n",
        "      userPrmpt['content'] += \"The Scene is still: \"\n",
        "\n",
        "    userPrmpt['content'] += current_scene + separator\n",
        "\n",
        "    recipients_str = \"\"\n",
        "    recipients = entry['Recipients']\n",
        "    if len(recipients) == 0:\n",
        "      recipients_str += \"to themselves\"\n",
        "    elif len(recipients) == 1:\n",
        "      recipients_str += recipients[0]\n",
        "    else:\n",
        "      recipients_str += \", \".join(recipients[:-1]) + \" and \" + recipients[-1]\n",
        "\n",
        "    userPrmpt['content'] += f\"{entry['Speaker']} says {quotation_token}{entry['Dialogue']}{quotation_token} to {recipients_str}.\"\n",
        "\n",
        "  else:\n",
        "    # no context\n",
        "    userPrmpt['content'] += \"Is the following line humorous or not humorous? Only respond '1' for humorous or '0' for not humorous: \"\n",
        "    userPrmpt['content'] += f\"{quotation_token}{entry['Dialogue']}{quotation_token}.\"\n",
        "\n",
        "  messages.append(userPrmpt)\n",
        "  return messages"
      ],
      "metadata": {
        "id": "3o6erjKqoOGl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Raw and Cleaned Formats:"
      ],
      "metadata": {
        "id": "vwRiMi5goqTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = \"/content/info4940-sitcom/cleaned-data/S1/The Big Bang_S0101.json\"\n",
        "nSentenceContext = 5\n",
        "\n",
        "with open(filePath, 'r') as file:\n",
        "  data = json.load(file)\n",
        "  data = [info for (_, info) in data.items()]\n",
        "\n",
        "print(\"NCONTEXT: \", nSentenceContext)\n",
        "\n",
        "print(f\"Example raw entry:\")\n",
        "display(data[nSentenceContext])\n",
        "\n",
        "print(f\"\\n\\nExample formatted entry ({nSentenceContext}context):\\n\")\n",
        "input_ = transformInput(data[:nSentenceContext], data[nSentenceContext])\n",
        "\n",
        "for i in input_:\n",
        "  print(f\"Role: {i['role']} | Content:\\n{i['content']}\\n\")\n",
        "\n",
        "print(f'Actual: {(\"isHumor\" in data[3])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "4BqYgUVUopXp",
        "outputId": "36e79107-d4b7-4ade-f381-66f44d76db5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCONTEXT:  5\n",
            "Example raw entry:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'EpisodeID': 'The Big Bang_S0101',\n",
              " 'Scene': 'A corridor at a sperm bank.',\n",
              " 'Recipients': ['Sheldon', 'Receptionist'],\n",
              " 'Speaker': 'Leonard',\n",
              " 'Dialogue': '14 down is... Move your finger...',\n",
              " 'Dialogue Start Time': '00:00:35:120000',\n",
              " 'Dialogue End Time': '00:00:37:190000',\n",
              " 'isHumor': True,\n",
              " 'humorDuration': '0:00:01'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Example formatted entry (5context):\n",
            "\n",
            "Role: system | Content:\n",
            "You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\n",
            "\n",
            "Role: user | Content:\n",
            "Use the following as context:\n",
            "\n",
            "The Scene is: A corridor at a sperm bank.\n",
            "Sheldon says \"So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slits. If it's unobserved, it will. However, if it's observed after it's left the plane but before it hits its target, it won't have gone through both slits.\" to Leonard.\n",
            "Leonard says \"Agreed. What's your point?\" to Sheldon.\n",
            "Sheldon says \"There's no point, I just think it's a good idea for a T-shirt.\" to Leonard.\n",
            "Leonard says \"Excuse me.\" to Sheldon and Receptionist.\n",
            "Leonard says \"One across is Aegean. Eight down is Nabokov. 26 across is MCM.\" to Sheldon and Receptionist.\n",
            "\n",
            "\n",
            "Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
            "\n",
            "The Scene is still: A corridor at a sperm bank.\n",
            "Leonard says \"14 down is... Move your finger...\" to Sheldon and Receptionist.\n",
            "\n",
            "Actual: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvksf8rFsnXP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating sample data"
      ],
      "metadata": {
        "id": "Hl-I2IVHmZd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePaths = []\n",
        "for i in range(5):\n",
        "  fPath = '/content/info4940-sitcom/cleaned-data/S' + str(i+1)\n",
        "  filePaths += list_files(fPath)\n",
        "\n",
        "# getting random number for random state for consistency?\n",
        "rand_seed1 = 64 # random.randint(1,100)\n",
        "rand_seed2 = 94 # random.randint(1,100)\n",
        "rand_seed3 = 69 # random.randint(1,100)\n",
        "\n",
        "# print([rand_seed1,rand_seed2,rand_seed3])\n",
        "\n",
        "# sampling 10 episodes\n",
        "nSamples = 10\n",
        "test_size = (len(filePaths) - nSamples) / len(filePaths)\n",
        "samplePaths, _ = train_test_split(filePaths, test_size=test_size, random_state=rand_seed1)\n",
        "\n",
        "# cleaning/transforming inputs\n",
        "nContext = 5\n",
        "nTests = 100\n",
        "\n",
        "humorousData, nonHumorousData = fPathList_TO_DialogueWithContext(samplePaths, nContext)\n",
        "sample_inputs, sample_labels = sampleEven(humorousData=humorousData, nonHumorousData=nonHumorousData, sample_size_per_class=(nTests/2), rand_seed=rand_seed2)\n",
        "\n",
        "# shuffling, not necessary but helpful for watching in-progress results\n",
        "rand_order = [i for i in range(len(sample_inputs))]\n",
        "random.seed(rand_seed3)\n",
        "random.shuffle(rand_order)\n",
        "\n",
        "inputs = [sample_inputs[i] for i in rand_order]\n",
        "labels = [sample_labels[i] for i in rand_order]"
      ],
      "metadata": {
        "id": "vIvtflQ5mYca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample input"
      ],
      "metadata": {
        "id": "FD7tw9OwnQ9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = []\n",
        "for i in range(len(inputs)):\n",
        "  print(inputs[i][1][\"content\"])\n",
        "  response = \"\"\n",
        "  while response not in [\"1\", \"0\"]:\n",
        "    response = input(\"Answer: \")\n",
        "  responses.append(response)"
      ],
      "metadata": {
        "id": "S7HvXap3Jth5",
        "outputId": "b6c95021-6b9c-4305-ad12-a49aa48191be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following as context:\n",
            "\n",
            "The Scene is: The cafeteria.\n",
            "Sheldon says \"Professor Jones told the symposium he had a new method for calculating the mass of a muon. Five times the limit of (laughs) 'E' to the upsilon as...\" to Howard, Leonard and Raj.\n",
            "Sheldon says \"Okay. No, no. I'll start over. Professor...\" to Howard, Leonard and Raj.\n",
            "Howard says \"I haven't seen him laugh that hard since the day Leonard made that multiplication error.\" to Sheldon, Leonard and Raj.\n",
            "Sheldon says \"Oh, Oh, Lord, that multiplication error! He thought he carried the one. But he didn't.\" to Howard, Leonard and Raj.\n",
            "Leonard says \"It's not funny. That mistake got published.\" to Sheldon, Howard and Raj.\n",
            "\n",
            "\n",
            "Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
            "\n",
            "The Scene is still: The cafeteria.\n",
            "Sheldon says \"Stop! I'm going to wet myself!\" to Howard, Leonard and Raj.\n",
            "Answer: d\n",
            "Answer: 4\n",
            "Answer: 1\n",
            "Use the following as context:\n",
            "\n",
            "The Scene is: The Cheesecake Factory.\n",
            "Waitress says \"Homeless, crazy guy at table 18.\" to Penny and Sheldon.\n",
            "Penny says \"No, just crazy.\" to Waitress and Sheldon.\n",
            "Penny says \"Sheldon, what are you doing here?\" to Waitress and Sheldon.\n",
            "Sheldon says \"I'm sick. Thank you very much.\" to Waitress and Penny.\n",
            "Penny says \"How could you have gotten if from me? I'm not sick.\" to Waitress and Sheldon.\n",
            "\n",
            "\n",
            "Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
            "\n",
            "The Scene is still: The Cheesecake Factory.\n",
            "Sheldon says \"You're a carrier. All these people here are doomed. You're doomed!\" to Waitress and Penny.\n",
            "Answer: 2\n",
            "Answer: 0\n",
            "Use the following as context:\n",
            "\n",
            "The Scene is: Where indeed.\n",
            "Howard says \"No, she never mentioned it.\" to MrsWolowitz.\n",
            "Mrs Wolowitz says \"I bet she did and you didn't listen!\" to Howard and MrsWolowitz.\n",
            "Howard says \"Yeah, that's probably it. So, what do you think? Do you like her? She's great, huh?\" to MrsWolowitz.\n",
            "Mrs Wolowitz says \"She's a lovely girl! Cute as a button!\" to Howard and MrsWolowitz.\n",
            "Howard says \"That's good to hear, 'cause I've got some news.\" to MrsWolowitz.\n",
            "\n",
            "\n",
            "Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
            "\n",
            "The Scene is still: Where indeed.\n",
            "Mrs Wolowitz says \"I hope it's good news, because I've got nothing but disappointment in here!\" to Howard and MrsWolowitz.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9d863030f6a5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses"
      ],
      "metadata": {
        "id": "XSuxJHaRSA9X",
        "outputId": "cfae956e-ae17-487d-a7d2-f50d298a787a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '0']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1aCKBhsZSJ6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}