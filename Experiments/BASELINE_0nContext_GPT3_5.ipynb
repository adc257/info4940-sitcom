{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgreene114/CovidDataWork/blob/main/may11_0nContext_GPT3_5_BASELINE_200tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U-WsE7AlB33",
        "outputId": "6c057b65-318a-4fff-c82a-7b6f175acd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTztUAKRlp7E",
        "outputId": "54768310-581a-482d-d049-b22a04728b23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "X6d94esWlNn7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adc257/info4940-sitcom.git\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiCa_A65lKVN",
        "outputId": "a3302edb-9341-42bb-c56a-1a23b0f32c18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'info4940-sitcom' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining some helper functions"
      ],
      "metadata": {
        "id": "kwyri_r4mFhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files(start_path):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(start_path):\n",
        "        for file in files:\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    file_paths.sort()\n",
        "    return file_paths\n",
        "\n",
        "def fPathList_TO_DialogueWithContext(filePathList, nSentenceContext):\n",
        "  humorous_inputs = []\n",
        "  non_humorous_inputs = []\n",
        "\n",
        "  # formatted_inputs = []\n",
        "  # labels = []\n",
        "\n",
        "  for episodePath in filePathList:\n",
        "\n",
        "    with open(episodePath, 'r') as file:\n",
        "      data = json.load(file)\n",
        "      data = [info for (_, info) in data.items()]\n",
        "\n",
        "\n",
        "      i = nSentenceContext\n",
        "      while i < len(data):\n",
        "        context = [data[j] for j in range(i-nSentenceContext, i)]\n",
        "        target = data[i]\n",
        "\n",
        "        if 'isHumor' in target:\n",
        "          humorous_inputs.append(transformInput(context, target))\n",
        "        else:\n",
        "          non_humorous_inputs.append(transformInput(context, target))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "  return humorous_inputs, non_humorous_inputs\n",
        "\n",
        "def sampleEven(humorousData, nonHumorousData, sample_size_per_class, rand_seed):\n",
        "  sample_size_per_class = min(sample_size_per_class, len(humorousData), len(nonHumorousData))\n",
        "\n",
        "  sample_inputs = []\n",
        "  sample_labels = []\n",
        "\n",
        "  data = humorousData\n",
        "  test_size = (len(data) - sample_size_per_class) / len(data)\n",
        "  sample, _ = train_test_split(data, test_size=test_size, random_state=rand_seed)\n",
        "  sample_inputs += sample\n",
        "  sample_labels += [1]*len(sample)\n",
        "\n",
        "\n",
        "  data = nonHumorousData\n",
        "  test_size = (len(data) - sample_size_per_class) / len(data)\n",
        "  sample, _ = train_test_split(data, test_size=test_size, random_state=rand_seed)\n",
        "  sample_inputs += sample\n",
        "  sample_labels += [0]*len(sample)\n",
        "\n",
        "  return sample_inputs, sample_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "BT2c3Cm_mFuB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testGPT(inputs, labels, printResultsEveryN=50):\n",
        "  resultsData = {\n",
        "      \"truePos\": [],\n",
        "      \"trueNeg\": [],\n",
        "      \"falsePos\": [],\n",
        "      \"falseNeg\": [],\n",
        "      \"other\": [],\n",
        "  }\n",
        "\n",
        "  results = {\n",
        "      \"truePos\": 0,\n",
        "      \"trueNeg\": 0,\n",
        "      \"falsePos\": 0,\n",
        "      \"falseNeg\": 0,\n",
        "      \"other\": 0,\n",
        "  }\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  pred = []\n",
        "\n",
        "\n",
        "\n",
        "  # terminators = [\n",
        "  #     tokenizer.eos_token_id,\n",
        "  #     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  # ]\n",
        "\n",
        "  for i in range(len(inputs)):\n",
        "    input = inputs[i]\n",
        "    label = labels[i]\n",
        "\n",
        "    # input_ids = tokenizer.apply_chat_template(\n",
        "    #   input,\n",
        "    #   add_generation_prompt=True,\n",
        "    #   return_tensors=\"pt\"\n",
        "    # ).to(model.device)\n",
        "\n",
        "    # outputs = model.generate(\n",
        "    #   input_ids,\n",
        "    #   max_new_tokens=50,\n",
        "    #   eos_token_id=terminators,\n",
        "    #   do_sample=True,\n",
        "    #   temperature=0.6,\n",
        "    #   top_p=0.9,\n",
        "    #   num_return_sequences=1,\n",
        "    # )\n",
        "\n",
        "    # response = outputs[0][input_ids.shape[-1]:]\n",
        "    # response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=input\n",
        "    )\n",
        "\n",
        "    response = completion.choices[0].message.content\n",
        "    response = response.strip()\n",
        "\n",
        "\n",
        "    if str(response) == str(label):\n",
        "      # true\n",
        "      if str(label) == \"1\":\n",
        "        # pos\n",
        "        resultsData[\"truePos\"].append(input)\n",
        "        results[\"truePos\"] += 1\n",
        "      elif str(label) == \"0\":\n",
        "        # neg\n",
        "        resultsData[\"trueNeg\"].append(input)\n",
        "        results[\"trueNeg\"] += 1\n",
        "      else:\n",
        "        # other\n",
        "        resultsData[\"other\"].append(input)\n",
        "        results[\"other\"] += 1\n",
        "      correct += 1\n",
        "    else:\n",
        "      # false\n",
        "      if str(label) == \"1\":\n",
        "        # pos, pred neg\n",
        "        resultsData[\"falseNeg\"].append(input)\n",
        "        results[\"falseNeg\"] += 1\n",
        "      elif str(label) == \"0\":\n",
        "        # neg, pred pos\n",
        "        resultsData[\"falsePos\"].append(input)\n",
        "        results[\"falsePos\"] += 1\n",
        "      else:\n",
        "        # other\n",
        "        resultsData[\"other\"].append(input)\n",
        "        results[\"other\"] += 1\n",
        "    pred.append(response)\n",
        "    total += 1\n",
        "\n",
        "    if i % printResultsEveryN == 0:\n",
        "      print(f\"Test: {i+1} of  {len(inputs)}\")\n",
        "      print(f\"Success rate: {correct / total:.2%}\\n\")\n",
        "      print(\"current results:\", results)\n",
        "\n",
        "\n",
        "  print(f\"Final success rate: {correct / total:.2%}\")\n",
        "  print(\"final results:\", results)\n",
        "\n",
        "  # true = [str(i) for i in labels]\n",
        "  report = None # classification_report(y_pred=pred, y_true=true, target_names=[\"Non-Humorous\",\"Humorous\"])\n",
        "\n",
        "  return results, resultsData, pred, report"
      ],
      "metadata": {
        "id": "dx122flnPV9d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining input formatter"
      ],
      "metadata": {
        "id": "NV9T5H7RoN2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformInput(context, target):\n",
        "  messages = []\n",
        "\n",
        "  separator = \"\\n\"\n",
        "  quotation_token = '\"'\n",
        "\n",
        "  # formatted_input = \"\"\n",
        "\n",
        "  if (len(context) > 0):\n",
        "    # if context...\n",
        "    systemPrmpt = {\n",
        "        'role': 'system',\n",
        "        'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "    }\n",
        "  else:\n",
        "    # if no context...\n",
        "    systemPrmpt = {\n",
        "        'role': 'system',\n",
        "        'content': \"You are humor classification model that takes a given line and determines whether it was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "    }\n",
        "\n",
        "  # append systemPrmpt\n",
        "  messages.append(systemPrmpt)\n",
        "\n",
        "  systemPrmpt2 = {\n",
        "      'role': 'system',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  userPrmpt = {\n",
        "      'role': 'user',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  userPrmpt2 = {\n",
        "      'role': 'user',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  asstPrmpt = {\n",
        "      'role': 'assistant',\n",
        "      'content': \"\"\n",
        "  }\n",
        "\n",
        "  # append userPrmpt\n",
        "  # messages.append(userPrmpt)\n",
        "\n",
        "  if len(context) > 0:\n",
        "    # asstPrmpt['content'] += \"Can you provide some context to help me with this decision?\"\n",
        "    # append asstPrmpt\n",
        "    # messages.append(asstPrmpt)\n",
        "\n",
        "    userPrmpt['content'] += \"Use the following as context:\" + separator*2\n",
        "\n",
        "    # if entry['Scene'] != current_scene:\n",
        "    #   current_scene = entry[\"Scene\"]\n",
        "    # userPrmpt['content'] += \"The scene changes to: \"\n",
        "    # else:\n",
        "\n",
        "    entry = context[0]\n",
        "    current_scene = entry[\"Scene\"]\n",
        "    userPrmpt['content'] += \"The Scene is: \"\n",
        "    userPrmpt['content'] += current_scene + separator\n",
        "\n",
        "    for entry in context:\n",
        "      if current_scene != entry['Scene']:\n",
        "        current_scene = entry[\"Scene\"]\n",
        "        userPrmpt['content'] += \"The scene changes to: \" + current_scene + separator\n",
        "\n",
        "      recipients = entry['Recipients']\n",
        "      recipients_str = \"\"\n",
        "      if len(recipients) == 0:\n",
        "        recipients_str += \"to themselves\"\n",
        "      elif len(recipients) == 1:\n",
        "        recipients_str += recipients[0]\n",
        "      else:\n",
        "        recipients_str += \", \".join(recipients[:-1]) + \" and \" + recipients[-1]\n",
        "\n",
        "      userPrmpt['content'] += f\"{entry['Speaker']} says {quotation_token}{entry['Dialogue']}{quotation_token} to {recipients_str}.\"\n",
        "      userPrmpt['content'] += separator\n",
        "\n",
        "    # append userPrmpt2\n",
        "\n",
        "  entry = target\n",
        "  if len(context) > 0:\n",
        "    # with context\n",
        "    userPrmpt['content'] += separator * 2 + \"Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\"\n",
        "\n",
        "    current_scene = context[-1]['Scene']\n",
        "    userPrmpt['content'] += separator*2\n",
        "\n",
        "\n",
        "    if entry['Scene'] != current_scene:\n",
        "      current_scene = entry[\"Scene\"]\n",
        "      userPrmpt['content'] += \"The scene changes to: \"\n",
        "    else:\n",
        "      userPrmpt['content'] += \"The Scene is still: \"\n",
        "\n",
        "    userPrmpt['content'] += current_scene + separator\n",
        "\n",
        "    recipients_str = \"\"\n",
        "    recipients = entry['Recipients']\n",
        "    if len(recipients) == 0:\n",
        "      recipients_str += \"to themselves\"\n",
        "    elif len(recipients) == 1:\n",
        "      recipients_str += recipients[0]\n",
        "    else:\n",
        "      recipients_str += \", \".join(recipients[:-1]) + \" and \" + recipients[-1]\n",
        "\n",
        "    userPrmpt['content'] += f\"{entry['Speaker']} says {quotation_token}{entry['Dialogue']}{quotation_token} to {recipients_str}.\"\n",
        "\n",
        "  else:\n",
        "    # no context\n",
        "    userPrmpt['content'] += \"Is the following line humorous or not humorous? Only respond '1' for humorous or '0' for not humorous: \"\n",
        "    userPrmpt['content'] += f\"{quotation_token}{entry['Dialogue']}{quotation_token}.\"\n",
        "\n",
        "  messages.append(userPrmpt)\n",
        "  return messages"
      ],
      "metadata": {
        "id": "3o6erjKqoOGl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Raw and Cleaned Formats:"
      ],
      "metadata": {
        "id": "vwRiMi5goqTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = \"/content/info4940-sitcom/cleaned-data/S1/The Big Bang_S0101.json\"\n",
        "nSentenceContext = 0\n",
        "\n",
        "with open(filePath, 'r') as file:\n",
        "  data = json.load(file)\n",
        "  data = [info for (_, info) in data.items()]\n",
        "\n",
        "print(\"NCONTEXT: \", nSentenceContext)\n",
        "\n",
        "print(f\"Example raw entry:\")\n",
        "display(data[nSentenceContext])\n",
        "\n",
        "print(f\"\\n\\nExample formatted entry ({nSentenceContext}context):\\n\")\n",
        "input = transformInput(data[:nSentenceContext], data[nSentenceContext])\n",
        "\n",
        "for i in input:\n",
        "  print(f\"Role: {i['role']} | Content:\\n{i['content']}\\n\")\n",
        "\n",
        "print(f'Actual: {(\"isHumor\" in data[3])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "4BqYgUVUopXp",
        "outputId": "7a96aa49-e385-4cf1-cea0-0c68acad550a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCONTEXT:  0\n",
            "Example raw entry:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'EpisodeID': 'The Big Bang_S0101',\n",
              " 'Scene': 'A corridor at a sperm bank.',\n",
              " 'Recipients': ['Leonard'],\n",
              " 'Speaker': 'Sheldon',\n",
              " 'Dialogue': \"So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slits. If it's unobserved, it will. However, if it's observed after it's left the plane but before it hits its target, it won't have gone through both slits.\",\n",
              " 'Dialogue Start Time': '00:00:00:140000',\n",
              " 'Dialogue End Time': '00:00:12:140000'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Example formatted entry (0context):\n",
            "\n",
            "Role: system | Content:\n",
            "You are humor classification model that takes a given line and determines whether it was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\n",
            "\n",
            "Role: user | Content:\n",
            "Is the following line humorous or not humorous? Only respond '1' for humorous or '0' for not humorous: \"So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slits. If it's unobserved, it will. However, if it's observed after it's left the plane but before it hits its target, it won't have gone through both slits.\".\n",
            "\n",
            "Actual: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvksf8rFsnXP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating sample data"
      ],
      "metadata": {
        "id": "Hl-I2IVHmZd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePaths = []\n",
        "for i in range(5):\n",
        "  fPath = '/content/info4940-sitcom/cleaned-data/S' + str(i+1)\n",
        "  filePaths += list_files(fPath)\n",
        "\n",
        "# getting random number for random state for consistency?\n",
        "rand_seed1 = 64 # random.randint(1,100)\n",
        "rand_seed2 = 94 # random.randint(1,100)\n",
        "rand_seed3 = 69 # random.randint(1,100)\n",
        "\n",
        "# print([rand_seed1,rand_seed2,rand_seed3])\n",
        "\n",
        "# sampling 10 episodes\n",
        "nSamples = 10\n",
        "test_size = (len(filePaths) - nSamples) / len(filePaths)\n",
        "samplePaths, _ = train_test_split(filePaths, test_size=test_size, random_state=rand_seed1)\n",
        "\n",
        "# cleaning/transforming inputs\n",
        "nContext = nSentenceContext\n",
        "nTests = 200\n",
        "\n",
        "humorousData, nonHumorousData = fPathList_TO_DialogueWithContext(samplePaths, nContext)\n",
        "sample_inputs, sample_labels = sampleEven(humorousData=humorousData, nonHumorousData=nonHumorousData, sample_size_per_class=(nTests/2), rand_seed=rand_seed2)\n",
        "\n",
        "# shuffling, not necessary but helpful for watching in-progress results\n",
        "rand_order = [i for i in range(len(sample_inputs))]\n",
        "random.seed(rand_seed3)\n",
        "random.shuffle(rand_order)\n",
        "\n",
        "inputs = [sample_inputs[i] for i in rand_order]\n",
        "labels = [sample_labels[i] for i in rand_order]"
      ],
      "metadata": {
        "id": "vIvtflQ5mYca"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample input"
      ],
      "metadata": {
        "id": "FD7tw9OwnQ9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj6IfHolnPn_",
        "outputId": "4fe84888-f740-4d76-eac3-064ab6185a5e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are humor classification model that takes a given line and determines whether it was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Is the following line humorous or not humorous? Only respond \\'1\\' for humorous or \\'0\\' for not humorous: \"Oh, Oh, Lord, that multiplication error! He thought he carried the one. But he didn\\'t.\".'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The GPT3.5 instruct model"
      ],
      "metadata": {
        "id": "JUQoH8FxlbSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get('OPENAI_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "JBB4C75RmrCK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "Pxbtspidmnqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsL2xH-MqETV",
        "outputId": "12d98263-9d82-45e0-8bb0-fe83bac76d82"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Use the following as context:\\n\\nThe Scene is: A corridor at a sperm bank.\\nSheldon says \"So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slits. If it\\'s unobserved, it will. However, if it\\'s observed after it\\'s left the plane but before it hits its target, it won\\'t have gone through both slits.\" to Leonard.\\nLeonard says \"Agreed. What\\'s your point?\" to Sheldon.\\nSheldon says \"There\\'s no point, I just think it\\'s a good idea for a T-shirt.\" to Leonard.\\nLeonard says \"Excuse me.\" to Sheldon and Receptionist.\\nLeonard says \"One across is Aegean. Eight down is Nabokov. 26 across is MCM.\" to Sheldon and Receptionist.\\n\\n\\nBased on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond \\'1\\' for humorous or \\'0\\' for not humorous:\\n\\nThe Scene is still: A corridor at a sperm bank.\\nLeonard says \"14 down is... Move your finger...\" to Sheldon and Receptionist.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# terminators = [\n",
        "#     tokenizer.eos_token_id,\n",
        "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]\n",
        "\n",
        "\n",
        "input = inputs[0]\n",
        "label = labels[0]\n",
        "\n",
        "# input_ids = tokenizer.apply_chat_template(\n",
        "#   input,\n",
        "#   add_generation_prompt=True,\n",
        "#   return_tensors=\"pt\"\n",
        "# ).to(model.device)\n",
        "\n",
        "# outputs = model.generate(\n",
        "#   input_ids,\n",
        "#   max_new_tokens=50,\n",
        "#   eos_token_id=terminators,\n",
        "#   do_sample=True,\n",
        "#   temperature=0.3,\n",
        "#   top_p=0.9,\n",
        "#   num_return_sequences=1,\n",
        "# )\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=input\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# response = outputs[0][input_ids.shape[-1]:]\n",
        "# response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkuB2zXkuvOq",
        "outputId": "3215cef0-9024-40ce-9edd-ff837cb27db4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHi9fiNXqOsk",
        "outputId": "7e2f3aa7-4186-4639-901c-f8b4ba30db25"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'role': 'system',\n",
              "   'content': \"You are humor classification model that takes a given line and determines whether it was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"},\n",
              "  {'role': 'user',\n",
              "   'content': 'Is the following line humorous or not humorous? Only respond \\'1\\' for humorous or \\'0\\' for not humorous: \"Oh, Oh, Lord, that multiplication error! He thought he carried the one. But he didn\\'t.\".'}],\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n",
        "print(f'expected: {label}, model: {response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohh4tsCdvotI",
        "outputId": "89a6d28c-d313-4d3a-fb2a-c8b62bfb78e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"}, {'role': 'user', 'content': 'Use the following as context:\\n\\nThe Scene is: The cafeteria.\\nSheldon says \"Professor Jones told the symposium he had a new method for calculating the mass of a muon. Five times the limit of (laughs) \\'E\\' to the upsilon as...\" to Howard, Leonard and Raj.\\nSheldon says \"Okay. No, no. I\\'ll start over. Professor...\" to Howard, Leonard and Raj.\\nHoward says \"I haven\\'t seen him laugh that hard since the day Leonard made that multiplication error.\" to Sheldon, Leonard and Raj.\\nSheldon says \"Oh, Oh, Lord, that multiplication error! He thought he carried the one. But he didn\\'t.\" to Howard, Leonard and Raj.\\nLeonard says \"It\\'s not funny. That mistake got published.\" to Sheldon, Howard and Raj.\\n\\n\\nBased on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond \\'1\\' for humorous or \\'0\\' for not humorous:\\n\\nThe Scene is still: The cafeteria.\\nSheldon says \"Stop! I\\'m going to wet myself!\" to Howard, Leonard and Raj.'}]\n",
            "expected: 1, model: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huVnwpSxoj1G",
        "outputId": "a7132d60-2bbd-499f-e748-645f7f7f88d3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 100, 0: 100})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set.*\")"
      ],
      "metadata": {
        "id": "sFQ85heIl-rK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nTests = 100\n",
        "print(\"NCONTEXT: \", nContext)\n",
        "print(\"NTESTS\", nTests)\n",
        "\n",
        "results, resultsData, pred, report = testGPT(inputs, labels, printResultsEveryN=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upyEZeGMmn3X",
        "outputId": "0821d344-f02a-4546-b89c-f5eac6010e34"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCONTEXT:  0\n",
            "NTESTS 200\n",
            "Test: 1 of  200\n",
            "Success rate: 100.00%\n",
            "\n",
            "current results: {'truePos': 1, 'trueNeg': 0, 'falsePos': 0, 'falseNeg': 0, 'other': 0}\n",
            "Test: 11 of  200\n",
            "Success rate: 54.55%\n",
            "\n",
            "current results: {'truePos': 4, 'trueNeg': 2, 'falsePos': 4, 'falseNeg': 1, 'other': 0}\n",
            "Test: 21 of  200\n",
            "Success rate: 61.90%\n",
            "\n",
            "current results: {'truePos': 11, 'trueNeg': 2, 'falsePos': 6, 'falseNeg': 2, 'other': 0}\n",
            "Test: 31 of  200\n",
            "Success rate: 48.39%\n",
            "\n",
            "current results: {'truePos': 13, 'trueNeg': 2, 'falsePos': 13, 'falseNeg': 3, 'other': 0}\n",
            "Test: 41 of  200\n",
            "Success rate: 56.10%\n",
            "\n",
            "current results: {'truePos': 18, 'trueNeg': 5, 'falsePos': 14, 'falseNeg': 4, 'other': 0}\n",
            "Test: 51 of  200\n",
            "Success rate: 54.90%\n",
            "\n",
            "current results: {'truePos': 23, 'trueNeg': 5, 'falsePos': 17, 'falseNeg': 6, 'other': 0}\n",
            "Test: 61 of  200\n",
            "Success rate: 49.18%\n",
            "\n",
            "current results: {'truePos': 24, 'trueNeg': 6, 'falsePos': 22, 'falseNeg': 9, 'other': 0}\n",
            "Test: 71 of  200\n",
            "Success rate: 52.11%\n",
            "\n",
            "current results: {'truePos': 28, 'trueNeg': 9, 'falsePos': 25, 'falseNeg': 9, 'other': 0}\n",
            "Test: 81 of  200\n",
            "Success rate: 51.85%\n",
            "\n",
            "current results: {'truePos': 32, 'trueNeg': 10, 'falsePos': 30, 'falseNeg': 9, 'other': 0}\n",
            "Test: 91 of  200\n",
            "Success rate: 50.55%\n",
            "\n",
            "current results: {'truePos': 35, 'trueNeg': 11, 'falsePos': 36, 'falseNeg': 9, 'other': 0}\n",
            "Test: 101 of  200\n",
            "Success rate: 49.50%\n",
            "\n",
            "current results: {'truePos': 38, 'trueNeg': 12, 'falsePos': 42, 'falseNeg': 9, 'other': 0}\n",
            "Test: 111 of  200\n",
            "Success rate: 48.65%\n",
            "\n",
            "current results: {'truePos': 41, 'trueNeg': 13, 'falsePos': 47, 'falseNeg': 10, 'other': 0}\n",
            "Test: 121 of  200\n",
            "Success rate: 52.07%\n",
            "\n",
            "current results: {'truePos': 48, 'trueNeg': 15, 'falsePos': 48, 'falseNeg': 10, 'other': 0}\n",
            "Test: 131 of  200\n",
            "Success rate: 50.38%\n",
            "\n",
            "current results: {'truePos': 51, 'trueNeg': 15, 'falsePos': 53, 'falseNeg': 12, 'other': 0}\n",
            "Test: 141 of  200\n",
            "Success rate: 51.77%\n",
            "\n",
            "current results: {'truePos': 58, 'trueNeg': 15, 'falsePos': 56, 'falseNeg': 12, 'other': 0}\n",
            "Test: 151 of  200\n",
            "Success rate: 51.66%\n",
            "\n",
            "current results: {'truePos': 62, 'trueNeg': 16, 'falsePos': 60, 'falseNeg': 13, 'other': 0}\n",
            "Test: 161 of  200\n",
            "Success rate: 53.42%\n",
            "\n",
            "current results: {'truePos': 68, 'trueNeg': 18, 'falsePos': 62, 'falseNeg': 13, 'other': 0}\n",
            "Test: 171 of  200\n",
            "Success rate: 54.97%\n",
            "\n",
            "current results: {'truePos': 72, 'trueNeg': 22, 'falsePos': 62, 'falseNeg': 15, 'other': 0}\n",
            "Test: 181 of  200\n",
            "Success rate: 56.35%\n",
            "\n",
            "current results: {'truePos': 78, 'trueNeg': 24, 'falsePos': 64, 'falseNeg': 15, 'other': 0}\n",
            "Test: 191 of  200\n",
            "Success rate: 54.45%\n",
            "\n",
            "current results: {'truePos': 79, 'trueNeg': 25, 'falsePos': 72, 'falseNeg': 15, 'other': 0}\n",
            "Final success rate: 55.50%\n",
            "final results: {'truePos': 84, 'trueNeg': 27, 'falsePos': 73, 'falseNeg': 16, 'other': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWbDDI-GsHOZ",
        "outputId": "76f7beef-e0c8-4060-b39c-2404b1ba20e1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'1': 156, '0': 43, '1 ': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "true = [str(i) for i in labels]\n",
        "print(\"N CONTEXT:\", nContext)\n",
        "report = classification_report(y_pred=pred, y_true=true, target_names=[\"Non-Humorous\",\"Humorous\"])\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0xTX238bqm",
        "outputId": "4ef6aab3-cd47-41fa-9e29-201a0cbf7974"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N CONTEXT: 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Non-Humorous       0.63      0.27      0.38       100\n",
            "    Humorous       0.54      0.84      0.65       100\n",
            "\n",
            "    accuracy                           0.56       200\n",
            "   macro avg       0.58      0.55      0.52       200\n",
            "weighted avg       0.58      0.56      0.52       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some interesting outputs along the way:\n",
        "\n",
        "## user: \"**What do you know about the sitcom the Big Bang Theory?**\"\n",
        "\n",
        "A popular and iconic sitcom! The Big Bang Theory is a American sitcom that originally aired from 2007 to 2019. Created by Chuck Lorre and Bill Prady, the show is a comedy about a group of socially awkward and intellectually brilliant scientists and their neighbor, a waitress and aspiring actress.\n",
        "\n",
        "The show revolves around the lives of five main characters:\n",
        "\n",
        "1. Sheldon Cooper (played by Jim Parsons), a theoretical physicist with Asperger's syndrome and a strict adherence to routine.\n",
        "2. Leonard Hofstadter (played by Johnny Galecki), an experimental physicist and Sheldon's roommate and best friend.\n",
        "3. Howard Wolowitz (played by Simon Helberg), an aerospace engineer and fellow physicist who lives across the hall from Leonard and Sheldon.\n",
        "4. Rajesh \"Raj\" Koothrappali (played by Kunal Nayyar), an astrophysicist who is unable to speak in the presence of women unless he drinks alcohol.\n",
        "5. Penny (played by Kaley Cuoco), a waitress and aspiring actress who lives across the hall from Leonard and Sheldon.\n",
        "\n",
        "Throughout the show's 12 seasons, the characters navigate their personal and professional lives, often finding themselves in humorous and absurd situations. The show explores themes such as friendship, love, and the [cut off]\n",
        "\n",
        "-------------------\n",
        "\n",
        "## Prompt:\n",
        "system: \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "\n",
        "user: 'Is the following target_moment from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
        "\n",
        "The Scene is still: The Cafeteria\n",
        "Sheldon says \"It's not what it looks like.\" to Leonard.'\n",
        "\n",
        "assistant: 'Can you provide some context to help me with this decision?'\n",
        "\n",
        "user: 'Yes, use the following as context:\n",
        "\n",
        "The Scene is still: The Cafeteria\n",
        "Priya says \"My brother-- he's got a big crush on Bernadette.\" to Leonard.\n",
        "Leonard says \"You're moving back to India?\" to Priya.\n",
        "Leonard says \"What's going on?\" to Raj, Penny, Raj and Howard.\n",
        "Penny says \"It's-It's not what it looks like.\" to Raj, Leonard, Raj and Howard.Sheldon says \"What does it look like?\" to Raj, Leonard, Raj and Howard.\n",
        "\n",
        "\n",
        "## Model's Response:\n",
        "\n",
        "Based on the context, I would classify the target moment as humorous. The phrase \"It's not what it looks like\" is a common idiomatic expression, but in this scene, it's being used in a way that's reminiscent of a soap opera or a melodrama, which adds a layer of humor to the situation. The fact that Sheldon is asking \"What does it look like?\" also implies that he's being obtuse and unaware of the obvious, which is a characteristic that's often played for comedic effect in the show.\n"
      ],
      "metadata": {
        "id": "dOJHzK6f2J4g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7HvXap3Jth5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}