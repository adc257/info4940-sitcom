{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37361edd-f18b-455e-a306-289e42a489e4",
   "metadata": {},
   "source": [
    "# The Big Bang Theory Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e29354-f5cd-4a30-8cbe-abc0f4c50ed3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c043b3-5369-4a0d-a3ea-ccb6b5f74949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T08:05:04.798902Z",
     "start_time": "2024-03-05T08:05:04.795466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db49bb-fd2a-4501-a25f-1197f02b94f4",
   "metadata": {},
   "source": [
    "## Loading sample data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfef5af-e471-4797-bbe7-01202fbc59bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T08:05:05.268703Z",
     "start_time": "2024-03-05T08:05:05.258113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/DT_2/Raw/S1/The Big Bang_S0101.json\", 'r') as file:\n",
    "    s1e1Data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b43b6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T08:05:24.924965Z",
     "start_time": "2024-03-05T08:05:24.919430Z"
    }
   },
   "outputs": [],
   "source": [
    "# two different time formats in data, this cleans them into datetime objects\n",
    "def string2datetime(string):\n",
    "    try:\n",
    "        return datetime.strptime(string.strip(), '%H:%M:%S:%f')\n",
    "\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return datetime.strptime(string.strip(), '%H:%M:%S,%f')\n",
    "        \n",
    "        except ValueError:\n",
    "            return datetime.strptime(string.strip(), '%H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702690396350a53f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:35:24.260222Z",
     "start_time": "2024-03-05T16:35:24.251739Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datetime2String(date):\n",
    "    return datetime.strftime(date, '%H:%M:%S:%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc3b0f0527b098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:37:57.536929Z",
     "start_time": "2024-03-05T16:37:57.532605Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findHumor(file_path1, file_path2):\n",
    "    humor_dict = {}\n",
    "    for file_path in [file_path1, file_path2]:\n",
    "        # print(file_path.upper())\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            for dialog_num, info in data.items():\n",
    "                if info['GT'] == 1:\n",
    "                    # print(\"humor\")\n",
    "                    humor_start, humor_end = string2datetime(info[\"Humor Start Time\"]), string2datetime(info[\"Humor End Time\"])\n",
    "                    duration = humor_end - humor_start\n",
    "                    if timedelta(seconds=0) <= duration < timedelta(seconds=10):\n",
    "                        humor_dict[humor_start] = humor_end\n",
    "    return dict(sorted(humor_dict.items(), key=lambda x: x[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215f7c06-5d82-437c-9c37-aacd39bf9c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:37:47.482636Z",
     "start_time": "2024-03-05T16:37:47.476554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reorganize_data(file):\n",
    "    lines_dict = {}\n",
    "    for dialog_num, info in file.items():            \n",
    "        dialog_turns = [key for key in info if key.startswith('Dialog Turn')]\n",
    "        for turn in dialog_turns:\n",
    "            start_time = string2datetime(info[turn][\"Dialog Start time\"])\n",
    "            if start_time in lines_dict: \n",
    "                pass\n",
    "            else:\n",
    "                lines_dict[start_time] = {\n",
    "                    \"Scene\": info[\"Scene\"],\n",
    "                    \"Recipients\": info[turn][\"Recipients\"],\n",
    "                    \"Speaker\": info[turn][\"Speaker\"],\n",
    "                    \"Dialogue\": info[turn][\"Dialog\"],\n",
    "                    \"Dialogue Start Time\": string2datetime(info[turn][\"Dialog Start time\"]),\n",
    "                    \"Dialogue End Time\": string2datetime(info[turn][\"Dialog End time\"])\n",
    "                }\n",
    "\n",
    "    return dict(sorted(lines_dict.items(), key=lambda x: x[0]))\n",
    "\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d519c1cf1237708d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:38:01.249802Z",
     "start_time": "2024-03-05T16:38:01.212809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path1 = \"data/DT_2/Raw/S1/The Big Bang_S0101.json\"\n",
    "path2 = \"data/DT_3/Raw/S1/The Big Bang_S0101.json\"\n",
    "\n",
    "humor_dict = findHumor(path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e966afe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:38:01.819482Z",
     "start_time": "2024-03-05T16:38:01.816326Z"
    }
   },
   "outputs": [],
   "source": [
    "lines_dict = reorganize_data(s1e1Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d6da19b4f27516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:38:02.255472Z",
     "start_time": "2024-03-05T16:38:02.243734Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: set buffer\n",
    "buffer = timedelta(seconds=1)\n",
    "\n",
    "nHumorMissingLine = 0\n",
    "idxsHumorMissingLine = []\n",
    "\n",
    "humor_times = list(humor_dict.keys())\n",
    "dialog_times = list(lines_dict.keys())\n",
    "humorN = 0\n",
    "dialogN = 0\n",
    "while humorN < len(humor_dict):\n",
    "    found_humor = False\n",
    "    while dialogN < len(dialog_times) - 1 and not found_humor:\n",
    "        laugh_start = humor_times[humorN]\n",
    "        laugh_end = humor_dict[laugh_start]\n",
    "        duration = laugh_end - laugh_start            \n",
    "        \n",
    "        current_dialog_start = dialog_times[dialogN]\n",
    "        current_dialog_end = lines_dict[current_dialog_start]['Dialogue End Time']\n",
    "        next_dialog_start = dialog_times[dialogN + 1]\n",
    "\n",
    "        if current_dialog_start <= laugh_start < next_dialog_start and laugh_start <= (current_dialog_end + buffer):\n",
    "            lines_dict[current_dialog_start]['isHumor'] = True\n",
    "            lines_dict[current_dialog_start]['humorDuration'] = duration\n",
    "            \n",
    "            # print(\"Humor line: \", lines_dict[current_dialog_start]['Dialogue'])\n",
    "            # print(\"Humor duration: \", humor_dict[laugh_start] - laugh_start)\n",
    "            # print(\"Speaker: \", lines_dict[current_dialog_start]['Speaker'])\n",
    "            # # print(\"\\n\")\n",
    "            # print(f\"Dialogue start: {lines_dict[current_dialog_start]['Dialogue Start Time']}\")\n",
    "            # print(f\"Dialogue end: {lines_dict[current_dialog_start]['Dialogue End Time']}\")\n",
    "            # print(f\"Humor start: {laugh_start}\")\n",
    "            # print(\"Humor end: \", humor_dict[laugh_start])\n",
    "            # print(\"\\n\\n\")\n",
    "            found_humor = True\n",
    "        dialogN += 1\n",
    "\n",
    "    if not found_humor:\n",
    "        dialogN = 0\n",
    "        nHumorMissingLine += 1\n",
    "        idxsHumorMissingLine.append(humorN)\n",
    "\n",
    "    humorN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f15ce8aa866363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:38:03.081433Z",
     "start_time": "2024-03-05T16:38:03.064310Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# startTime and endTime must be in format String: \"HH:MM:SS\"\n",
    "def getLinesBetween(startTime, endTime, lines_dict):\n",
    "    \n",
    "    return {\n",
    "        lineStartTime: data\n",
    "        for lineStartTime, data in lines_dict.items()\n",
    "        if  string2datetime(startTime) <= lineStartTime <= string2datetime(endTime)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439ca1b61232326",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Instances where humor wasn't matched with a line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821fc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingHumorInfo(missingHumorIndex):\n",
    "    missingHumor = {}\n",
    "    before_time = None\n",
    "    after_time = None\n",
    "    for index in missingHumorIndex:\n",
    "        humorStart = humor_times[index]\n",
    "        humorEnd = humor_dict[humorStart]\n",
    "\n",
    "        for time_key in lines_dict.keys():\n",
    "            if time_key < humorStart:\n",
    "                before_time = time_key\n",
    "            elif time_key > humorStart:\n",
    "                after_time = time_key\n",
    "                break\n",
    "\n",
    "        missingHumor[humorStart] = {\"Humor Start\": humorStart,\n",
    "                                    \"Humor End\": humorEnd,\n",
    "                                    \"Humor Duration\":humorEnd-humorStart,\n",
    "                                    \"DialogEnd to HumorStart\": humorStart - lines_dict[before_time]['Dialogue End Time'],\n",
    "                                    \"HumorEnd to NextDialogStart\": lines_dict[after_time]['Dialogue Start Time'] - humorEnd\n",
    "                                    }\n",
    "        \n",
    "    return missingHumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d59cf5b2bc6825b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:38:06.120860Z",
     "start_time": "2024-03-05T16:38:06.117178Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missingHumorDict = missingHumorInfo(idxsHumorMissingLine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaada9a1ff6ffa8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ab7f48c3acc7bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:56:01.284204Z",
     "start_time": "2024-03-05T16:56:01.273102Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned-data/.DS_Store\n",
      "cleaned-data/S5/The Big Bang_S0511.json\n",
      "cleaned-data/S5/The Big Bang_S0507.json\n",
      "cleaned-data/S5/The Big Bang_S0506.json\n",
      "cleaned-data/S5/The Big Bang_S0510.json\n",
      "cleaned-data/S5/The Big Bang_S0521.json\n",
      "cleaned-data/S5/The Big Bang_S0517.json\n",
      "cleaned-data/S5/The Big Bang_S0501.json\n",
      "cleaned-data/S5/The Big Bang_S0516.json\n",
      "cleaned-data/S5/The Big Bang_S0520.json\n",
      "cleaned-data/S5/The Big Bang_S0523.json\n",
      "cleaned-data/S5/The Big Bang_S0519.json\n",
      "cleaned-data/S5/The Big Bang_S0503.json\n",
      "cleaned-data/S5/The Big Bang_S0515.json\n",
      "cleaned-data/S5/The Big Bang_S0514.json\n",
      "cleaned-data/S5/The Big Bang_S0502.json\n",
      "cleaned-data/S5/The Big Bang_S0518.json\n",
      "cleaned-data/S5/The Big Bang_S0522.json\n",
      "cleaned-data/S5/The Big Bang_S0505.json\n",
      "cleaned-data/S5/The Big Bang_S0513.json\n",
      "cleaned-data/S5/The Big Bang_S0509.json\n",
      "cleaned-data/S5/The Big Bang_S0508.json\n",
      "cleaned-data/S5/The Big Bang_S0512.json\n",
      "cleaned-data/S5/The Big Bang_S0504.json\n",
      "cleaned-data/S2/The Big Bang_S0205.json\n",
      "cleaned-data/S2/The Big Bang_S0213.json\n",
      "cleaned-data/S2/The Big Bang_S0209.json\n",
      "cleaned-data/S2/The Big Bang_S0208.json\n",
      "cleaned-data/S2/The Big Bang_S0212.json\n",
      "cleaned-data/S2/The Big Bang_S0204.json\n",
      "cleaned-data/S2/The Big Bang_S0223.json\n",
      "cleaned-data/S2/The Big Bang_S0219.json\n",
      "cleaned-data/S2/The Big Bang_S0203.json\n",
      "cleaned-data/S2/The Big Bang_S0215.json\n",
      "cleaned-data/S2/The Big Bang_S0214.json\n",
      "cleaned-data/S2/The Big Bang_S0202.json\n",
      "cleaned-data/S2/The Big Bang_S0218.json\n",
      "cleaned-data/S2/The Big Bang_S0222.json\n",
      "cleaned-data/S2/The Big Bang_S0221.json\n",
      "cleaned-data/S2/The Big Bang_S0217.json\n",
      "cleaned-data/S2/The Big Bang_S0201.json\n",
      "cleaned-data/S2/The Big Bang_S0216.json\n",
      "cleaned-data/S2/The Big Bang_S0220.json\n",
      "cleaned-data/S2/The Big Bang_S0211.json\n",
      "cleaned-data/S2/The Big Bang_S0207.json\n",
      "cleaned-data/S2/The Big Bang_S0206.json\n",
      "cleaned-data/S2/The Big Bang_S0210.json\n",
      "cleaned-data/S3/The Big Bang_S0317.json\n",
      "cleaned-data/S3/The Big Bang_S0301.json\n",
      "cleaned-data/S3/The Big Bang_S0321.json\n",
      "cleaned-data/S3/The Big Bang_S0320.json\n",
      "cleaned-data/S3/The Big Bang_S0316.json\n",
      "cleaned-data/S3/The Big Bang_S0311.json\n",
      "cleaned-data/S3/The Big Bang_S0307.json\n",
      "cleaned-data/S3/The Big Bang_S0306.json\n",
      "cleaned-data/S3/The Big Bang_S0310.json\n",
      "cleaned-data/S3/The Big Bang_S0309.json\n",
      "cleaned-data/S3/The Big Bang_S0305.json\n",
      "cleaned-data/S3/The Big Bang_S0313.json\n",
      "cleaned-data/S3/The Big Bang_S0312.json\n",
      "cleaned-data/S3/The Big Bang_S0304.json\n",
      "cleaned-data/S3/The Big Bang_S0308.json\n",
      "cleaned-data/S3/The Big Bang_S0303.json\n",
      "cleaned-data/S3/The Big Bang_S0315.json\n",
      "cleaned-data/S3/The Big Bang_S0319.json\n",
      "cleaned-data/S3/The Big Bang_S0323.json\n",
      "cleaned-data/S3/The Big Bang_S0322.json\n",
      "cleaned-data/S3/The Big Bang_S0318.json\n",
      "cleaned-data/S3/The Big Bang_S0314.json\n",
      "cleaned-data/S3/The Big Bang_S0302.json\n",
      "cleaned-data/S4/The Big Bang_S0403.json\n",
      "cleaned-data/S4/The Big Bang_S0415.json\n",
      "cleaned-data/S4/The Big Bang_S0419.json\n",
      "cleaned-data/S4/The Big Bang_S0423.json\n",
      "cleaned-data/S4/The Big Bang_S0422.json\n",
      "cleaned-data/S4/The Big Bang_S0418.json\n",
      "cleaned-data/S4/The Big Bang_S0414.json\n",
      "cleaned-data/S4/The Big Bang_S0402.json\n",
      "cleaned-data/S4/The Big Bang_S0409.json\n",
      "cleaned-data/S4/The Big Bang_S0405.json\n",
      "cleaned-data/S4/The Big Bang_S0413.json\n",
      "cleaned-data/S4/The Big Bang_S0412.json\n",
      "cleaned-data/S4/The Big Bang_S0404.json\n",
      "cleaned-data/S4/The Big Bang_S0424.json\n",
      "cleaned-data/S4/The Big Bang_S0408.json\n",
      "cleaned-data/S4/The Big Bang_S0411.json\n",
      "cleaned-data/S4/The Big Bang_S0407.json\n",
      "cleaned-data/S4/The Big Bang_S0406.json\n",
      "cleaned-data/S4/The Big Bang_S0410.json\n",
      "cleaned-data/S4/The Big Bang_S0417.json\n",
      "cleaned-data/S4/The Big Bang_S0401.json\n",
      "cleaned-data/S4/The Big Bang_S0421.json\n",
      "cleaned-data/S4/The Big Bang_S0420.json\n",
      "cleaned-data/S4/The Big Bang_S0416.json\n",
      "cleaned-data/S1/The Big Bang_S0116.json\n",
      "cleaned-data/S1/The Big Bang_S0101.json\n",
      "cleaned-data/S1/The Big Bang_S0117.json\n",
      "cleaned-data/S1/The Big Bang_S0110.json\n",
      "cleaned-data/S1/The Big Bang_S0106.json\n",
      "cleaned-data/S1/The Big Bang_S0107.json\n",
      "cleaned-data/S1/The Big Bang_S0111.json\n",
      "cleaned-data/S1/The Big Bang_S0108.json\n",
      "cleaned-data/S1/The Big Bang_S0104.json\n",
      "cleaned-data/S1/The Big Bang_S0112.json\n",
      "cleaned-data/S1/The Big Bang_S0113.json\n",
      "cleaned-data/S1/The Big Bang_S0105.json\n",
      "cleaned-data/S1/The Big Bang_S0109.json\n",
      "cleaned-data/S1/The Big Bang_S0102.json\n",
      "cleaned-data/S1/The Big Bang_S0114.json\n",
      "cleaned-data/S1/The Big Bang_S0115.json\n",
      "cleaned-data/S1/The Big Bang_S0103.json\n"
     ]
    }
   ],
   "source": [
    "directory = Path(\"data/DT_2/Raw\")\n",
    "\n",
    "for file_path in directory.rglob('*'):\n",
    "    if file_path.is_file():\n",
    "        with open(file_path, 'r') as rFile:\n",
    "            data = json.load(rFile)\n",
    "            humor_dict = findHumor(file_path)\n",
    "            lines_dict = reorganize_data(data)\n",
    "        \n",
    "        new_file_path = \"cleaned-\" + str(file_path).replace(\"/DT_2/Raw\", \"\")\n",
    "        \n",
    "        with open(new_file_path, 'w') as wFile:\n",
    "            json.dump(lines_dict)\n",
    "    # if not directory.exists():\n",
    "    #     directory.mkdir(parents=True, exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
