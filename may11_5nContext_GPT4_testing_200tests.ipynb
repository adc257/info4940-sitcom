{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adc257/info4940-sitcom/blob/main/may11_5nContext_GPT4_testing_200tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U-WsE7AlB33",
        "outputId": "6c057b65-318a-4fff-c82a-7b6f175acd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTztUAKRlp7E",
        "outputId": "54768310-581a-482d-d049-b22a04728b23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "X6d94esWlNn7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adc257/info4940-sitcom.git\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiCa_A65lKVN",
        "outputId": "a3302edb-9341-42bb-c56a-1a23b0f32c18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'info4940-sitcom' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining some helper functions"
      ],
      "metadata": {
        "id": "kwyri_r4mFhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files(start_path):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(start_path):\n",
        "        for file in files:\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    file_paths.sort()\n",
        "    return file_paths\n",
        "\n",
        "def fPathList_TO_DialogueWithContext(filePathList, nSentenceContext):\n",
        "  humorous_inputs = []\n",
        "  non_humorous_inputs = []\n",
        "\n",
        "  # formatted_inputs = []\n",
        "  # labels = []\n",
        "\n",
        "  for episodePath in filePathList:\n",
        "\n",
        "    with open(episodePath, 'r') as file:\n",
        "      data = json.load(file)\n",
        "      data = [info for (_, info) in data.items()]\n",
        "\n",
        "\n",
        "      i = nSentenceContext\n",
        "      while i < len(data):\n",
        "        context = [data[j] for j in range(i-nSentenceContext, i)]\n",
        "        target = data[i]\n",
        "\n",
        "        if 'isHumor' in target:\n",
        "          humorous_inputs.append(transformInput(context, target))\n",
        "        else:\n",
        "          non_humorous_inputs.append(transformInput(context, target))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "  return humorous_inputs, non_humorous_inputs\n",
        "\n",
        "def sampleEven(humorousData, nonHumorousData, sample_size_per_class, rand_seed):\n",
        "  sample_size_per_class = min(sample_size_per_class, len(humorousData), len(nonHumorousData))\n",
        "\n",
        "  sample_inputs = []\n",
        "  sample_labels = []\n",
        "\n",
        "  data = humorousData\n",
        "  test_size = (len(data) - sample_size_per_class) / len(data)\n",
        "  sample, _ = train_test_split(data, test_size=test_size, random_state=rand_seed)\n",
        "  sample_inputs += sample\n",
        "  sample_labels += [1]*len(sample)\n",
        "\n",
        "\n",
        "  data = nonHumorousData\n",
        "  test_size = (len(data) - sample_size_per_class) / len(data)\n",
        "  sample, _ = train_test_split(data, test_size=test_size, random_state=rand_seed)\n",
        "  sample_inputs += sample\n",
        "  sample_labels += [0]*len(sample)\n",
        "\n",
        "  return sample_inputs, sample_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "BT2c3Cm_mFuB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testGPT(inputs, labels, printResultsEveryN=50):\n",
        "  resultsData = {\n",
        "      \"truePos\": [],\n",
        "      \"trueNeg\": [],\n",
        "      \"falsePos\": [],\n",
        "      \"falseNeg\": [],\n",
        "      \"other\": [],\n",
        "  }\n",
        "\n",
        "  results = {\n",
        "      \"truePos\": 0,\n",
        "      \"trueNeg\": 0,\n",
        "      \"falsePos\": 0,\n",
        "      \"falseNeg\": 0,\n",
        "      \"other\": 0,\n",
        "  }\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  pred = []\n",
        "\n",
        "\n",
        "\n",
        "  # terminators = [\n",
        "  #     tokenizer.eos_token_id,\n",
        "  #     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  # ]\n",
        "\n",
        "  for i in range(len(inputs)):\n",
        "    input = inputs[i]\n",
        "    label = labels[i]\n",
        "\n",
        "    # input_ids = tokenizer.apply_chat_template(\n",
        "    #   input,\n",
        "    #   add_generation_prompt=True,\n",
        "    #   return_tensors=\"pt\"\n",
        "    # ).to(model.device)\n",
        "\n",
        "    # outputs = model.generate(\n",
        "    #   input_ids,\n",
        "    #   max_new_tokens=50,\n",
        "    #   eos_token_id=terminators,\n",
        "    #   do_sample=True,\n",
        "    #   temperature=0.6,\n",
        "    #   top_p=0.9,\n",
        "    #   num_return_sequences=1,\n",
        "    # )\n",
        "\n",
        "    # response = outputs[0][input_ids.shape[-1]:]\n",
        "    # response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-4-turbo\",\n",
        "      messages=input\n",
        "    )\n",
        "\n",
        "    response = completion.choices[0].message.content\n",
        "    response = response.strip()\n",
        "\n",
        "\n",
        "    if str(response) == str(label):\n",
        "      # true\n",
        "      if str(label) == \"1\":\n",
        "        # pos\n",
        "        resultsData[\"truePos\"].append(input)\n",
        "        results[\"truePos\"] += 1\n",
        "      elif str(label) == \"0\":\n",
        "        # neg\n",
        "        resultsData[\"trueNeg\"].append(input)\n",
        "        results[\"trueNeg\"] += 1\n",
        "      else:\n",
        "        # other\n",
        "        resultsData[\"other\"].append(input)\n",
        "        results[\"other\"] += 1\n",
        "      correct += 1\n",
        "    else:\n",
        "      # false\n",
        "      if str(label) == \"1\":\n",
        "        # pos, pred neg\n",
        "        resultsData[\"falseNeg\"].append(input)\n",
        "        results[\"falseNeg\"] += 1\n",
        "      elif str(label) == \"0\":\n",
        "        # neg, pred pos\n",
        "        resultsData[\"falsePos\"].append(input)\n",
        "        results[\"falsePos\"] += 1\n",
        "      else:\n",
        "        # other\n",
        "        resultsData[\"other\"].append(input)\n",
        "        results[\"other\"] += 1\n",
        "    pred.append(response)\n",
        "    total += 1\n",
        "\n",
        "    if i % printResultsEveryN == 0:\n",
        "      print(f\"Test: {i+1} of  {len(inputs)}\")\n",
        "      print(f\"Success rate: {correct / total:.2%}\\n\")\n",
        "      print(\"current results:\", results)\n",
        "\n",
        "\n",
        "  print(f\"Final success rate: {correct / total:.2%}\")\n",
        "  print(\"final results:\", results)\n",
        "\n",
        "  # true = [str(i) for i in labels]\n",
        "  report = None # classification_report(y_pred=pred, y_true=true, target_names=[\"Non-Humorous\",\"Humorous\"])\n",
        "\n",
        "  return results, resultsData, pred, report"
      ],
      "metadata": {
        "id": "dx122flnPV9d"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining input formatter"
      ],
      "metadata": {
        "id": "NV9T5H7RoN2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformInput(context, target):\n",
        "  messages = []\n",
        "\n",
        "  separator = \"\\n\"\n",
        "  quotation_token = '\"'\n",
        "\n",
        "  # formatted_input = \"\"\n",
        "\n",
        "  if (len(context) > 0):\n",
        "    # if context...\n",
        "    systemPrmpt = {\n",
        "        'role': 'system',\n",
        "        'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "    }\n",
        "  else:\n",
        "    # if no context...\n",
        "    systemPrmpt = {\n",
        "        'role': 'system',\n",
        "        'content': \"You are humor classification model that takes a given line and determines whether it was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "    }\n",
        "\n",
        "  # append systemPrmpt\n",
        "  messages.append(systemPrmpt)\n",
        "\n",
        "  systemPrmpt2 = {\n",
        "      'role': 'system',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  userPrmpt = {\n",
        "      'role': 'user',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  userPrmpt2 = {\n",
        "      'role': 'user',\n",
        "      'content': \"\"\n",
        "  }\n",
        "  asstPrmpt = {\n",
        "      'role': 'assistant',\n",
        "      'content': \"\"\n",
        "  }\n",
        "\n",
        "  # append userPrmpt\n",
        "  # messages.append(userPrmpt)\n",
        "\n",
        "  if len(context) > 0:\n",
        "    # asstPrmpt['content'] += \"Can you provide some context to help me with this decision?\"\n",
        "    # append asstPrmpt\n",
        "    # messages.append(asstPrmpt)\n",
        "\n",
        "    userPrmpt['content'] += \"Use the following as context:\" + separator*2\n",
        "\n",
        "    # if entry['Scene'] != current_scene:\n",
        "    #   current_scene = entry[\"Scene\"]\n",
        "    # userPrmpt['content'] += \"The scene changes to: \"\n",
        "    # else:\n",
        "\n",
        "    entry = context[0]\n",
        "    current_scene = entry[\"Scene\"]\n",
        "    userPrmpt['content'] += \"The Scene is: \"\n",
        "    userPrmpt['content'] += current_scene + separator\n",
        "\n",
        "    for entry in context:\n",
        "      if current_scene != entry['Scene']:\n",
        "        current_scene = entry[\"Scene\"]\n",
        "        userPrmpt['content'] += \"The scene changes to: \" + current_scene + separator\n",
        "\n",
        "      recipients = entry['Recipients']\n",
        "      recipients_str = \"\"\n",
        "      if len(recipients) == 0:\n",
        "        recipients_str += \"to themselves\"\n",
        "      elif len(recipients) == 1:\n",
        "        recipients_str += recipients[0]\n",
        "      else:\n",
        "        recipients_str += \", \".join(recipients[:-1]) + \" and \" + recipients[-1]\n",
        "\n",
        "      userPrmpt['content'] += f\"{entry['Speaker']} says {quotation_token}{entry['Dialogue']}{quotation_token} to {recipients_str}.\"\n",
        "      userPrmpt['content'] += separator\n",
        "\n",
        "    # append userPrmpt2\n",
        "\n",
        "  entry = target\n",
        "  if len(context) > 0:\n",
        "    # with context\n",
        "    userPrmpt['content'] += separator * 2 + \"Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\"\n",
        "\n",
        "    current_scene = context[-1]['Scene']\n",
        "    userPrmpt['content'] += separator*2\n",
        "\n",
        "\n",
        "    if entry['Scene'] != current_scene:\n",
        "      current_scene = entry[\"Scene\"]\n",
        "      userPrmpt['content'] += \"The scene changes to: \"\n",
        "    else:\n",
        "      userPrmpt['content'] += \"The Scene is still: \"\n",
        "\n",
        "    userPrmpt['content'] += current_scene + separator\n",
        "\n",
        "    recipients_str = \"\"\n",
        "    recipients = entry['Recipients']\n",
        "    if len(recipients) == 0:\n",
        "      recipients_str += \"to themselves\"\n",
        "    elif len(recipients) == 1:\n",
        "      recipients_str += recipients[0]\n",
        "    else:\n",
        "      recipients_str += \", \".join(recipients[:-1]) + \" and \" + recipients[-1]\n",
        "\n",
        "    userPrmpt['content'] += f\"{entry['Speaker']} says {quotation_token}{entry['Dialogue']}{quotation_token} to {recipients_str}.\"\n",
        "\n",
        "  else:\n",
        "    # no context\n",
        "    userPrmpt['content'] += \"Is the following line humorous or not humorous? Only respond '1' for humorous or '0' for not humorous: \"\n",
        "    userPrmpt['content'] += f\"{quotation_token}{entry['Dialogue']}{quotation_token}.\"\n",
        "\n",
        "  messages.append(userPrmpt)\n",
        "  return messages"
      ],
      "metadata": {
        "id": "3o6erjKqoOGl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Raw and Cleaned Formats:"
      ],
      "metadata": {
        "id": "vwRiMi5goqTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = \"/content/info4940-sitcom/cleaned-data/S1/The Big Bang_S0101.json\"\n",
        "nSentenceContext = 5\n",
        "\n",
        "with open(filePath, 'r') as file:\n",
        "  data = json.load(file)\n",
        "  data = [info for (_, info) in data.items()]\n",
        "\n",
        "print(\"NCONTEXT: \", nSentenceContext)\n",
        "\n",
        "print(f\"Example raw entry:\")\n",
        "display(data[nSentenceContext])\n",
        "\n",
        "print(f\"\\n\\nExample formatted entry ({nSentenceContext}context):\\n\")\n",
        "input = transformInput(data[:nSentenceContext], data[nSentenceContext])\n",
        "\n",
        "for i in input:\n",
        "  print(f\"Role: {i['role']} | Content:\\n{i['content']}\\n\")\n",
        "\n",
        "print(f'Actual: {(\"isHumor\" in data[3])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "4BqYgUVUopXp",
        "outputId": "61443484-448c-4780-b6af-cd40ad7df940"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCONTEXT:  5\n",
            "Example raw entry:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'EpisodeID': 'The Big Bang_S0101',\n",
              " 'Scene': 'A corridor at a sperm bank.',\n",
              " 'Recipients': ['Sheldon', 'Receptionist'],\n",
              " 'Speaker': 'Leonard',\n",
              " 'Dialogue': '14 down is... Move your finger...',\n",
              " 'Dialogue Start Time': '00:00:35:120000',\n",
              " 'Dialogue End Time': '00:00:37:190000',\n",
              " 'isHumor': True,\n",
              " 'humorDuration': '0:00:01'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Example formatted entry (5context):\n",
            "\n",
            "Role: system | Content:\n",
            "You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\n",
            "\n",
            "Role: user | Content:\n",
            "Use the following as context:\n",
            "\n",
            "The Scene is: A corridor at a sperm bank.\n",
            "Sheldon says \"So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slits. If it's unobserved, it will. However, if it's observed after it's left the plane but before it hits its target, it won't have gone through both slits.\" to Leonard.\n",
            "Leonard says \"Agreed. What's your point?\" to Sheldon.\n",
            "Sheldon says \"There's no point, I just think it's a good idea for a T-shirt.\" to Leonard.\n",
            "Leonard says \"Excuse me.\" to Sheldon and Receptionist.\n",
            "Leonard says \"One across is Aegean. Eight down is Nabokov. 26 across is MCM.\" to Sheldon and Receptionist.\n",
            "\n",
            "\n",
            "Based on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
            "\n",
            "The Scene is still: A corridor at a sperm bank.\n",
            "Leonard says \"14 down is... Move your finger...\" to Sheldon and Receptionist.\n",
            "\n",
            "Actual: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating sample data"
      ],
      "metadata": {
        "id": "Hl-I2IVHmZd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePaths = []\n",
        "for i in range(5):\n",
        "  fPath = '/content/info4940-sitcom/cleaned-data/S' + str(i+1)\n",
        "  filePaths += list_files(fPath)\n",
        "\n",
        "# getting random number for random state for consistency?\n",
        "rand_seed1 = 64 # random.randint(1,100)\n",
        "rand_seed2 = 94 # random.randint(1,100)\n",
        "rand_seed3 = 69 # random.randint(1,100)\n",
        "\n",
        "# print([rand_seed1,rand_seed2,rand_seed3])\n",
        "\n",
        "# sampling 10 episodes\n",
        "nSamples = 10\n",
        "test_size = (len(filePaths) - nSamples) / len(filePaths)\n",
        "samplePaths, _ = train_test_split(filePaths, test_size=test_size, random_state=rand_seed1)\n",
        "\n",
        "# cleaning/transforming inputs\n",
        "nContext = nSentenceContext\n",
        "nTests = 200\n",
        "\n",
        "humorousData, nonHumorousData = fPathList_TO_DialogueWithContext(samplePaths, nContext)\n",
        "sample_inputs, sample_labels = sampleEven(humorousData=humorousData, nonHumorousData=nonHumorousData, sample_size_per_class=(nTests/2), rand_seed=rand_seed2)\n",
        "\n",
        "# shuffling, not necessary but helpful for watching in-progress results\n",
        "rand_order = [i for i in range(len(sample_inputs))]\n",
        "random.seed(rand_seed3)\n",
        "random.shuffle(rand_order)\n",
        "\n",
        "inputs = [sample_inputs[i] for i in rand_order]\n",
        "labels = [sample_labels[i] for i in rand_order]"
      ],
      "metadata": {
        "id": "vIvtflQ5mYca"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample input"
      ],
      "metadata": {
        "id": "FD7tw9OwnQ9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj6IfHolnPn_",
        "outputId": "977f38e2-c634-4369-ae56-6ebbc151487b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Use the following as context:\\n\\nThe Scene is: Amy’s laboratory.\\nSheldon says \"It\\'s not. I\\'ll have you know, in the field of physics, we work with particles so small, they make fat jokes about the locus coeruleus.\" to Amy.\\nSheldon says \"I.e., when your locus coeruleus sits around the house, it sits around the house.\" to Amy.\\nAmy says \"Oh... are we nervous, Dr. Cooper?\" to Sheldon.\\nSheldon says \"No. What you see is a man trembling with confidence.\" to Amy.\\nSheldon says \"Does a locus coeruleus normally bleed that much?\" to Amy.\\n\\n\\nBased on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond \\'1\\' for humorous or \\'0\\' for not humorous:\\n\\nThe Scene is still: Amy’s laboratory.\\nAmy says \"No. But your thumb does.\" to Sheldon.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The GPT3.5 instruct model"
      ],
      "metadata": {
        "id": "JUQoH8FxlbSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get('OPENAI_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "JBB4C75RmrCK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "Pxbtspidmnqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# terminators = [\n",
        "#     tokenizer.eos_token_id,\n",
        "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]\n",
        "\n",
        "\n",
        "input = inputs[0]\n",
        "label = labels[0]\n",
        "\n",
        "# input_ids = tokenizer.apply_chat_template(\n",
        "#   input,\n",
        "#   add_generation_prompt=True,\n",
        "#   return_tensors=\"pt\"\n",
        "# ).to(model.device)\n",
        "\n",
        "# outputs = model.generate(\n",
        "#   input_ids,\n",
        "#   max_new_tokens=50,\n",
        "#   eos_token_id=terminators,\n",
        "#   do_sample=True,\n",
        "#   temperature=0.3,\n",
        "#   top_p=0.9,\n",
        "#   num_return_sequences=1,\n",
        "# )\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4-turbo\",\n",
        "  messages=input\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# response = outputs[0][input_ids.shape[-1]:]\n",
        "# response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkuB2zXkuvOq",
        "outputId": "73195c3b-0d35-4cd3-a33c-8bd7dcf100f2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHi9fiNXqOsk",
        "outputId": "1a030dc2-d430-4359-c2cb-3e2bf77d146b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'role': 'system',\n",
              "   'content': \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"},\n",
              "  {'role': 'user',\n",
              "   'content': 'Use the following as context:\\n\\nThe Scene is: Amy’s laboratory.\\nSheldon says \"It\\'s not. I\\'ll have you know, in the field of physics, we work with particles so small, they make fat jokes about the locus coeruleus.\" to Amy.\\nSheldon says \"I.e., when your locus coeruleus sits around the house, it sits around the house.\" to Amy.\\nAmy says \"Oh... are we nervous, Dr. Cooper?\" to Sheldon.\\nSheldon says \"No. What you see is a man trembling with confidence.\" to Amy.\\nSheldon says \"Does a locus coeruleus normally bleed that much?\" to Amy.\\n\\n\\nBased on the context provided, is the following line from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond \\'1\\' for humorous or \\'0\\' for not humorous:\\n\\nThe Scene is still: Amy’s laboratory.\\nAmy says \"No. But your thumb does.\" to Sheldon.'}],\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n",
        "print(f'expected: {label}, model: {response}')"
      ],
      "metadata": {
        "id": "Ohh4tsCdvotI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huVnwpSxoj1G",
        "outputId": "ab0bc73b-4279-4346-bb58-1110ade5bb5a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 100, 0: 100})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set.*\")"
      ],
      "metadata": {
        "id": "sFQ85heIl-rK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nTests = 100\n",
        "print(\"NCONTEXT: \", nContext)\n",
        "print(\"NTESTS\", nTests)\n",
        "\n",
        "results, resultsData, pred, report = testGPT(inputs, labels, printResultsEveryN=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upyEZeGMmn3X",
        "outputId": "87b2760e-03a8-421b-9bab-13aa1955327a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCONTEXT:  5\n",
            "NTESTS 200\n",
            "Test: 1 of  200\n",
            "Success rate: 100.00%\n",
            "\n",
            "current results: {'truePos': 1, 'trueNeg': 0, 'falsePos': 0, 'falseNeg': 0, 'other': 0}\n",
            "Test: 11 of  200\n",
            "Success rate: 54.55%\n",
            "\n",
            "current results: {'truePos': 3, 'trueNeg': 3, 'falsePos': 3, 'falseNeg': 2, 'other': 0}\n",
            "Test: 21 of  200\n",
            "Success rate: 61.90%\n",
            "\n",
            "current results: {'truePos': 9, 'trueNeg': 4, 'falsePos': 4, 'falseNeg': 4, 'other': 0}\n",
            "Test: 31 of  200\n",
            "Success rate: 54.84%\n",
            "\n",
            "current results: {'truePos': 12, 'trueNeg': 5, 'falsePos': 10, 'falseNeg': 4, 'other': 0}\n",
            "Test: 41 of  200\n",
            "Success rate: 63.41%\n",
            "\n",
            "current results: {'truePos': 18, 'trueNeg': 8, 'falsePos': 11, 'falseNeg': 4, 'other': 0}\n",
            "Test: 51 of  200\n",
            "Success rate: 64.71%\n",
            "\n",
            "current results: {'truePos': 24, 'trueNeg': 9, 'falsePos': 13, 'falseNeg': 5, 'other': 0}\n",
            "Test: 61 of  200\n",
            "Success rate: 65.57%\n",
            "\n",
            "current results: {'truePos': 28, 'trueNeg': 12, 'falsePos': 16, 'falseNeg': 5, 'other': 0}\n",
            "Test: 71 of  200\n",
            "Success rate: 64.79%\n",
            "\n",
            "current results: {'truePos': 31, 'trueNeg': 15, 'falsePos': 19, 'falseNeg': 6, 'other': 0}\n",
            "Test: 81 of  200\n",
            "Success rate: 64.20%\n",
            "\n",
            "current results: {'truePos': 35, 'trueNeg': 17, 'falsePos': 23, 'falseNeg': 6, 'other': 0}\n",
            "Test: 91 of  200\n",
            "Success rate: 62.64%\n",
            "\n",
            "current results: {'truePos': 36, 'trueNeg': 21, 'falsePos': 26, 'falseNeg': 8, 'other': 0}\n",
            "Test: 101 of  200\n",
            "Success rate: 58.42%\n",
            "\n",
            "current results: {'truePos': 37, 'trueNeg': 22, 'falsePos': 32, 'falseNeg': 10, 'other': 0}\n",
            "Test: 111 of  200\n",
            "Success rate: 58.56%\n",
            "\n",
            "current results: {'truePos': 39, 'trueNeg': 26, 'falsePos': 34, 'falseNeg': 12, 'other': 0}\n",
            "Test: 121 of  200\n",
            "Success rate: 60.33%\n",
            "\n",
            "current results: {'truePos': 45, 'trueNeg': 28, 'falsePos': 35, 'falseNeg': 13, 'other': 0}\n",
            "Test: 131 of  200\n",
            "Success rate: 61.07%\n",
            "\n",
            "current results: {'truePos': 50, 'trueNeg': 30, 'falsePos': 38, 'falseNeg': 13, 'other': 0}\n",
            "Test: 141 of  200\n",
            "Success rate: 61.70%\n",
            "\n",
            "current results: {'truePos': 55, 'trueNeg': 32, 'falsePos': 39, 'falseNeg': 15, 'other': 0}\n",
            "Test: 151 of  200\n",
            "Success rate: 62.25%\n",
            "\n",
            "current results: {'truePos': 58, 'trueNeg': 36, 'falsePos': 40, 'falseNeg': 17, 'other': 0}\n",
            "Test: 161 of  200\n",
            "Success rate: 62.11%\n",
            "\n",
            "current results: {'truePos': 62, 'trueNeg': 38, 'falsePos': 42, 'falseNeg': 19, 'other': 0}\n",
            "Test: 171 of  200\n",
            "Success rate: 63.74%\n",
            "\n",
            "current results: {'truePos': 67, 'trueNeg': 42, 'falsePos': 42, 'falseNeg': 20, 'other': 0}\n",
            "Test: 181 of  200\n",
            "Success rate: 64.64%\n",
            "\n",
            "current results: {'truePos': 71, 'trueNeg': 46, 'falsePos': 42, 'falseNeg': 22, 'other': 0}\n",
            "Test: 191 of  200\n",
            "Success rate: 64.40%\n",
            "\n",
            "current results: {'truePos': 72, 'trueNeg': 51, 'falsePos': 46, 'falseNeg': 22, 'other': 0}\n",
            "Final success rate: 65.50%\n",
            "final results: {'truePos': 78, 'trueNeg': 53, 'falsePos': 47, 'falseNeg': 22, 'other': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "true = [str(i) for i in labels]\n",
        "print(\"N CONTEXT:\", nContext)\n",
        "report = classification_report(y_pred=pred, y_true=true, target_names=[\"Non-Humorous\",\"Humorous\"])\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0xTX238bqm",
        "outputId": "626034d6-ca50-4e85-88ce-782e7c3c8e36"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N CONTEXT: 5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Non-Humorous       0.71      0.53      0.61       100\n",
            "    Humorous       0.62      0.78      0.69       100\n",
            "\n",
            "    accuracy                           0.66       200\n",
            "   macro avg       0.67      0.66      0.65       200\n",
            "weighted avg       0.67      0.66      0.65       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some interesting outputs along the way:\n",
        "\n",
        "## user: \"**What do you know about the sitcom the Big Bang Theory?**\"\n",
        "\n",
        "A popular and iconic sitcom! The Big Bang Theory is a American sitcom that originally aired from 2007 to 2019. Created by Chuck Lorre and Bill Prady, the show is a comedy about a group of socially awkward and intellectually brilliant scientists and their neighbor, a waitress and aspiring actress.\n",
        "\n",
        "The show revolves around the lives of five main characters:\n",
        "\n",
        "1. Sheldon Cooper (played by Jim Parsons), a theoretical physicist with Asperger's syndrome and a strict adherence to routine.\n",
        "2. Leonard Hofstadter (played by Johnny Galecki), an experimental physicist and Sheldon's roommate and best friend.\n",
        "3. Howard Wolowitz (played by Simon Helberg), an aerospace engineer and fellow physicist who lives across the hall from Leonard and Sheldon.\n",
        "4. Rajesh \"Raj\" Koothrappali (played by Kunal Nayyar), an astrophysicist who is unable to speak in the presence of women unless he drinks alcohol.\n",
        "5. Penny (played by Kaley Cuoco), a waitress and aspiring actress who lives across the hall from Leonard and Sheldon.\n",
        "\n",
        "Throughout the show's 12 seasons, the characters navigate their personal and professional lives, often finding themselves in humorous and absurd situations. The show explores themes such as friendship, love, and the [cut off]\n",
        "\n",
        "-------------------\n",
        "\n",
        "## Prompt:\n",
        "system: \"You are humor classification model that takes a given line (the target) along with several precending lines (the context), and determines whether the target line was humorous (return '1') or not humorous (return '0'). RESTRICTION: ONLY RESPOND 1 OR 0.\"\n",
        "\n",
        "user: 'Is the following target_moment from the sitcom, the Big Bang Theory, humorous or not humorous? Only respond '1' for humorous or '0' for not humorous:\n",
        "\n",
        "The Scene is still: The Cafeteria\n",
        "Sheldon says \"It's not what it looks like.\" to Leonard.'\n",
        "\n",
        "assistant: 'Can you provide some context to help me with this decision?'\n",
        "\n",
        "user: 'Yes, use the following as context:\n",
        "\n",
        "The Scene is still: The Cafeteria\n",
        "Priya says \"My brother-- he's got a big crush on Bernadette.\" to Leonard.\n",
        "Leonard says \"You're moving back to India?\" to Priya.\n",
        "Leonard says \"What's going on?\" to Raj, Penny, Raj and Howard.\n",
        "Penny says \"It's-It's not what it looks like.\" to Raj, Leonard, Raj and Howard.Sheldon says \"What does it look like?\" to Raj, Leonard, Raj and Howard.\n",
        "\n",
        "\n",
        "## Model's Response:\n",
        "\n",
        "Based on the context, I would classify the target moment as humorous. The phrase \"It's not what it looks like\" is a common idiomatic expression, but in this scene, it's being used in a way that's reminiscent of a soap opera or a melodrama, which adds a layer of humor to the situation. The fact that Sheldon is asking \"What does it look like?\" also implies that he's being obtuse and unaware of the obvious, which is a characteristic that's often played for comedic effect in the show.\n"
      ],
      "metadata": {
        "id": "dOJHzK6f2J4g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7HvXap3Jth5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}