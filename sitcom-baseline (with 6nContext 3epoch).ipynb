{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "37361edd-f18b-455e-a306-289e42a489e4",
      "metadata": {
        "id": "37361edd-f18b-455e-a306-289e42a489e4"
      },
      "source": [
        "# The Big Bang Theory Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e29354-f5cd-4a30-8cbe-abc0f4c50ed3",
      "metadata": {
        "tags": [],
        "id": "91e29354-f5cd-4a30-8cbe-abc0f4c50ed3"
      },
      "source": [
        "### Imports:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSbyj3J0ugVj",
        "outputId": "00ce8a6b-faa4-4f49-9e86-091321926fb5"
      },
      "id": "LSbyj3J0ugVj",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adc257/info4940-sitcom.git"
      ],
      "metadata": {
        "id": "BrUbqiIntXth",
        "outputId": "38146aee-5030-421a-a222-2f96ddaba0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BrUbqiIntXth",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'info4940-sitcom' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENEIQ-sCPJfb",
        "outputId": "4f7a6767-a095-4f0f-9e2c-1017b01f36b2"
      },
      "id": "ENEIQ-sCPJfb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m venv 3350\n",
        "!source 3350/bin/activate"
      ],
      "metadata": {
        "id": "jKPk26plSJNy"
      },
      "id": "jKPk26plSJNy",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QrJXWkoibGD",
        "outputId": "f2881660-4575-45b7-dacc-e19172d61c73"
      },
      "id": "3QrJXWkoibGD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/site-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (2.2.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/site-packages (from transformers[torch]) (0.28.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/site-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install transformers[torch]\n",
        "# !pip install accelerate -U\n",
        "# !pip install accelerate>=0.21.0\n",
        "# !pip install --upgrade pip"
      ],
      "metadata": {
        "id": "-wa9rUhV4HWM"
      },
      "id": "-wa9rUhV4HWM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip uninstall transformers accelerate\n",
        "# pip install transformers[torch]\n",
        "# !pip install --upgrade setuptools"
      ],
      "metadata": {
        "id": "zftPtQ7PNNeT"
      },
      "id": "zftPtQ7PNNeT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5c043b3-5369-4a0d-a3ea-ccb6b5f74949",
      "metadata": {
        "tags": [],
        "id": "f5c043b3-5369-4a0d-a3ea-ccb6b5f74949",
        "ExecuteTime": {
          "end_time": "2024-03-18T17:36:54.219587Z",
          "start_time": "2024-03-18T17:36:54.211176Z"
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta, time\n",
        "from pathlib import Path\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import os\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HO9kUogg4F8-",
        "ExecuteTime": {
          "end_time": "2024-03-18T17:36:57.363182Z",
          "start_time": "2024-03-18T17:36:57.339429Z"
        }
      },
      "id": "HO9kUogg4F8-",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "51db49bb-fd2a-4501-a25f-1197f02b94f4",
      "metadata": {
        "id": "51db49bb-fd2a-4501-a25f-1197f02b94f4"
      },
      "source": [
        "## Loading sample data file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e533b5d1",
      "metadata": {
        "id": "e533b5d1",
        "ExecuteTime": {
          "end_time": "2024-03-18T17:36:58.611344Z",
          "start_time": "2024-03-18T17:36:58.587028Z"
        }
      },
      "outputs": [],
      "source": [
        "def list_files(start_path):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(start_path):\n",
        "        for file in files:\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    file_paths.sort()\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generateDialogueWithContext(filePath, nSentenceContext):\n",
        "  dialogue_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for episodePath in list_files(filePath):\n",
        "\n",
        "    with open(episodePath, 'r') as file:\n",
        "      data = json.load(file)\n",
        "      data = [info for (_, info) in data.items()]\n",
        "\n",
        "      i = nSentenceContext\n",
        "      while i < len(data):\n",
        "        dialog = \"\"\n",
        "        for j in range(i-nSentenceContext, i):\n",
        "          _info = data[j]\n",
        "          dialog += str(_info['Dialogue']) + \" \"\n",
        "\n",
        "        info = data[i]\n",
        "        # add in tag where context changes to target line\n",
        "        # dialog += \"[CONTEXT|LINE]\"\n",
        "        dialog += str(info['Dialogue'])\n",
        "\n",
        "        dialogue_list.append(dialog)\n",
        "\n",
        "        if \"isHumor\" in info:\n",
        "          label_list.append(1)\n",
        "        else:\n",
        "          label_list.append(0)\n",
        "\n",
        "        i += 1\n",
        "  return dialogue_list, label_list"
      ],
      "metadata": {
        "id": "QKV3_0Hc6M1L"
      },
      "id": "QKV3_0Hc6M1L",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue_list1, label_list1 = generateDialogueWithContext('/content/info4940-sitcom/cleaned-data/S1', 6)\n",
        "dialogue_list2, label_list2 = generateDialogueWithContext('/content/info4940-sitcom/cleaned-data/S2', 6)\n",
        "dialogue_list3, label_list3 = generateDialogueWithContext('/content/info4940-sitcom/cleaned-data/S3', 6)\n",
        "dialogue_list4, label_list4 = generateDialogueWithContext('/content/info4940-sitcom/cleaned-data/S4', 6)\n",
        "dialogue_list5, label_list5 = generateDialogueWithContext('/content/info4940-sitcom/cleaned-data/S5', 6)"
      ],
      "metadata": {
        "id": "RnvrEKjEGGqY"
      },
      "id": "RnvrEKjEGGqY",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = dialogue_list1 + dialogue_list2 + dialogue_list3 + dialogue_list4\n",
        "test_texts = dialogue_list5\n",
        "train_labels = label_list1 + label_list2 + label_list3 + label_list4\n",
        "test_labels = label_list5"
      ],
      "metadata": {
        "id": "mQDtMqo1Hn2F"
      },
      "id": "mQDtMqo1Hn2F",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if all the dialogues are strings:\n",
        "\n",
        "# for i in range(len(test_texts)):\n",
        "#   if type(test_texts[i]) != str:\n",
        "#     print(\"NOT A STRING\")\n",
        "#     print(\"index:\",i)\n",
        "#     print(\"string:\",test_texts[i])\n",
        "\n",
        "\n",
        "\n",
        "# Change this Dialogue from a number to a string\n",
        "# test_texts[2106] = \"1863.0\""
      ],
      "metadata": {
        "id": "9LGu8RbPJRJB"
      },
      "id": "9LGu8RbPJRJB",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline:"
      ],
      "metadata": {
        "id": "xEidNKdmYjQo"
      },
      "id": "xEidNKdmYjQo"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "RxL91CnsYitU"
      },
      "id": "RxL91CnsYitU",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True,lowercase=True,stop_words='english')\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)"
      ],
      "metadata": {
        "id": "ne4seJeXZW1n"
      },
      "id": "ne4seJeXZW1n",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "predictions = []\n",
        "clf = RandomForestClassifier(max_depth=5,n_jobs=-1).fit(X_train, train_labels)\n",
        "\n",
        "predictions.append(clf.predict(X_test))\n",
        "scores.append(clf.score(X_test, test_labels))"
      ],
      "metadata": {
        "id": "pdbK2eZRYdrM"
      },
      "id": "pdbK2eZRYdrM",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Cross validated score: ', np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STyphdNfY_N-",
        "outputId": "f22cda99-17ab-4102-e0de-8db73ee5fcf8"
      },
      "id": "STyphdNfY_N-",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross validated score:  0.5377838684416602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Text Embeddings:"
      ],
      "metadata": {
        "id": "JecHN7wK9su-"
      },
      "id": "JecHN7wK9su-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Data for BERT:\n",
        "model_name = 'distilbert-base-cased'\n",
        "device_name = 'cuda' # (you can use 'cpu' or 'mps')\n",
        "max_length = 512\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings  = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "      self.encodings = encodings\n",
        "      self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "      item['labels'] = torch.tensor(self.labels[idx])\n",
        "      return item\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, train_labels)\n",
        "test_dataset = MyDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "zc4KaDuFNjt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74db9143-7b6d-4e3d-e164-0d8c5d350c19"
      },
      "id": "zc4KaDuFNjt2",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pre-trained BERT Model:\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                            num_labels=2).to(device_name)"
      ],
      "metadata": {
        "id": "YpQK_lcWPcrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e733cc-1cfa-4b4b-d848-c3ee3de9628b"
      },
      "id": "YpQK_lcWPcrq",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 3\n",
        "total_training_steps = len(train_dataset) * num_train_epochs\n",
        "warmup_proportion = 0.1"
      ],
      "metadata": {
        "id": "YM6ZBv58cdE_"
      },
      "id": "YM6ZBv58cdE_",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcOKhssZiYxD"
      },
      "id": "kcOKhssZiYxD",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "  num_train_epochs=num_train_epochs,              # total number of training epochs\n",
        "  per_device_train_batch_size=16,  # batch size per device during training\n",
        "  per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "  learning_rate=5e-5,              # initial learning rate for Adam optimizer\n",
        "  warmup_steps= 1000,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
        "  weight_decay=0.01,               # strength of weight decay\n",
        "  output_dir='./results',          # output directory\n",
        "  logging_dir='./logs',            # directory for storing logs\n",
        "  logging_steps= 0.2,               # number of steps to output logging (set lower because of small dataset size)\n",
        "  evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
        ")"
      ],
      "metadata": {
        "id": "aJVB8YzrgvsJ"
      },
      "id": "aJVB8YzrgvsJ",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tuning our BERT model:\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  #score = accuracy_score(labels, preds)\n",
        "  score = f1_score(labels, preds, average='weighted')\n",
        "  return {\n",
        "      'f1': score,\n",
        "  }"
      ],
      "metadata": {
        "id": "tYHRlKsjgz8Z"
      },
      "id": "tYHRlKsjgz8Z",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=test_dataset,\n",
        "  compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "hAlpX7BJg8u6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "6d9afc13-2bd4-4b14-db5e-6171e7b856dd"
      },
      "id": "hAlpX7BJg8u6",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3738' max='3738' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3738/3738 31:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.662200</td>\n",
              "      <td>0.688622</td>\n",
              "      <td>0.378681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1496</td>\n",
              "      <td>0.643700</td>\n",
              "      <td>0.648692</td>\n",
              "      <td>0.617239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2244</td>\n",
              "      <td>0.597300</td>\n",
              "      <td>0.622530</td>\n",
              "      <td>0.654461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2992</td>\n",
              "      <td>0.495600</td>\n",
              "      <td>0.715857</td>\n",
              "      <td>0.639190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results/checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3738, training_loss=0.5634165152054282, metrics={'train_runtime': 1886.9049, 'train_samples_per_second': 31.692, 'train_steps_per_second': 1.981, 'total_flos': 4981829271597864.0, 'train_loss': 0.5634165152054282, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cached_model_directory_name = 'distill-bert-tuned-with-context'\n",
        "trainer.save_model(cached_model_directory_name)"
      ],
      "metadata": {
        "id": "N2qOgPFEg_u_"
      },
      "id": "N2qOgPFEg_u_",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the model above:\n",
        "# current_directory = os.getcwd()\n",
        "# model_directory = os.path.join(current_directory, cached_model_directory_name)\n",
        "\n",
        "# saved_model_directory = \"/path/to/your/directory/distill-bert-tuned-no-context\"\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_directory)"
      ],
      "metadata": {
        "id": "k66745wihyiX"
      },
      "id": "k66745wihyiX",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating our Fine-Tuned Model:\n",
        "trainer.evaluate()\n",
        "predicted_results = trainer.predict(test_dataset)\n",
        "\n",
        "\n",
        "predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
        "predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
        "\n",
        "print(classification_report(test_labels, predicted_labels))\n",
        "print(classification_report(test_labels, predicted_labels, output_dict = True)['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "egttSBk-hFhF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "c7ca4497-6448-42ee-91cc-c0af4534a93e"
      },
      "id": "egttSBk-hFhF",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.79      0.71      2747\n",
            "           1       0.67      0.49      0.56      2361\n",
            "\n",
            "    accuracy                           0.65      5108\n",
            "   macro avg       0.65      0.64      0.64      5108\n",
            "weighted avg       0.65      0.65      0.64      5108\n",
            "\n",
            "0.641086660295939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "01S1aEKGis6s"
      },
      "id": "01S1aEKGis6s",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}